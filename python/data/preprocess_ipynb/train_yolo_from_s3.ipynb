{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473c6b58",
   "metadata": {},
   "source": [
    "# YOLOãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆS3ãƒ™ãƒ¼ã‚¹ï¼‰\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€`preprocess_raw_to_yolo_and_pack.ipynb`ã§ä½œæˆã—ã¦S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸYOLOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã€**YOLOv5nãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’**ã—ã€å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’S3ã«ä¿å­˜ã—ã¾ã™ã€‚\n",
    "\n",
    "**é‡è¦**: ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯**YOLOv5ã®å…¬å¼ãƒªãƒã‚¸ãƒˆãƒªï¼ˆultralytics/yolov5ï¼‰**ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã‚’è¡Œã„ã¾ã™ã€‚`ultralytics`ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼ˆYOLOv8ï¼‰ã§ã¯ãªãã€ç¢ºå®Ÿã«YOLOv5ã§å­¦ç¿’ã§ãã¾ã™ã€‚\n",
    "\n",
    "## å‡¦ç†ã®æµã‚Œ\n",
    "1. AWSèªè¨¼è¨­å®š\n",
    "2. YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "3. S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å±•é–‹\n",
    "4. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèª\n",
    "5. data.yamlã®ä½œæˆ\n",
    "6. å­¦ç¿’è¨­å®š\n",
    "7. YOLOv5ã®train.pyã§å­¦ç¿’ã®å®Ÿè¡Œ\n",
    "8. å­¦ç¿’çµæœã®ç¢ºèª\n",
    "9. ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "## å‰ææ¡ä»¶\n",
    "- `preprocess_raw_to_yolo_and_pack.ipynb`ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã€S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã‚ã‚‹ã“ã¨\n",
    "- GPUãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã§å®Ÿè¡Œã™ã‚‹ã“ã¨ï¼ˆæ¨å¥¨ï¼‰\n",
    "- GitãŒåˆ©ç”¨å¯èƒ½ã§ã‚ã‚‹ã“ã¨ï¼ˆYOLOv5ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ã«å¿…è¦ï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456088e",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«0: AWSèªè¨¼è¨­å®š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eae555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAxxxxxxxxxxxxxxxx\"  # åŸ‹ã‚ã¦ã­ï¼ï¼ï¼ï¼ï¼ï¼ï¼\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"xxxxxxxxxxxxxxxxxxxx\"  # åŸ‹ã‚ã¦ã­ï¼ï¼ï¼ï¼ï¼ï¼ï¼\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"ap-northeast-1\"\n",
    "\n",
    "print(\"AWS env set\")\n",
    "\n",
    "assert \"AWS_ACCESS_KEY_ID\" in os.environ, \"AWS_ACCESS_KEY_ID not set\"\n",
    "assert \"AWS_SECRET_ACCESS_KEY\" in os.environ, \"AWS_SECRET_ACCESS_KEY not set\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e6247",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«1: è¨­å®šï¼ˆã“ã“ã ã‘è§¦ã‚Œã°OKï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# è¨­å®šï¼ˆã“ã“ã ã‘å¤‰æ›´ã™ã‚Œã°OKï¼‰\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== S3è¨­å®š =====\n",
    "S3_BUCKET = \"km62m-ml-storage\"  # S3ãƒã‚±ãƒƒãƒˆå\n",
    "S3_PREFIX = \"yolo-dataset/v1\"  # S3ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆpreprocess_raw_to_yolo_and_pack.ipynbã§ä½¿ç”¨ã—ãŸã‚‚ã®ã¨åŒã˜ï¼‰\n",
    "\n",
    "# ===== ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹è¨­å®š =====\n",
    "LOCAL_DATA_DIR = Path(\"/content/tmp/yolo_dataset\")  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å±•é–‹å…ˆ\n",
    "LOCAL_OUTPUT_DIR = Path(\"/content/tmp/yolo_training\")  # å­¦ç¿’çµæœã®å‡ºåŠ›å…ˆ\n",
    "\n",
    "# ===== å­¦ç¿’è¨­å®š =====\n",
    "EPOCHS = 50  # ã‚¨ãƒãƒƒã‚¯æ•°\n",
    "BATCH_SIZE = 16  # ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆGPUãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´ï¼‰\n",
    "IMG_SIZE = 640  # ç”»åƒã‚µã‚¤ã‚º\n",
    "DEVICE = 0  # GPUä½¿ç”¨ï¼ˆ0: GPU, 'cpu': CPUï¼‰\n",
    "\n",
    "# ===== ãƒ¢ãƒ‡ãƒ«è¨­å®š =====\n",
    "MODEL_NAME = \"yolov5n\"  # YOLOv5nãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\n",
    "PRETRAINED_WEIGHTS = \"yolov5n.pt\"  # äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿\n",
    "\n",
    "# ===== ãã®ä»– =====\n",
    "CLEAN_LOCAL_DATA = False  # Trueã«ã™ã‚‹ã¨æ—¢å­˜ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤\n",
    "\n",
    "print(\"âœ“ è¨­å®šå®Œäº†\")\n",
    "print(f\"  S3_BUCKET: {S3_BUCKET}\")\n",
    "print(f\"  S3_PREFIX: {S3_PREFIX}\")\n",
    "print(f\"  LOCAL_DATA_DIR: {LOCAL_DATA_DIR}\")\n",
    "print(f\"  EPOCHS: {EPOCHS}\")\n",
    "print(f\"  BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"  IMG_SIZE: {IMG_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf3f89f",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«2: ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc294a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ã¨ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# ============================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "YOLOV5_DIR = Path(\"/content/yolov5\")\n",
    "\n",
    "# YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ï¼ˆæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
    "if YOLOV5_DIR.exists() and (YOLOV5_DIR / \"train.py\").exists():\n",
    "    print(f\"\\nâœ“ YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã¯æ—¢ã«ã‚¯ãƒ­ãƒ¼ãƒ³æ¸ˆã¿: {YOLOV5_DIR}\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“¦ YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­...\")\n",
    "    print(\"   URL: https://github.com/ultralytics/yolov5\")\n",
    "    \n",
    "    # æ—¢å­˜ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚Œã°å‰Šé™¤\n",
    "    if YOLOV5_DIR.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(YOLOV5_DIR)\n",
    "    \n",
    "    subprocess.check_call([\n",
    "        \"git\", \"clone\", \n",
    "        \"https://github.com/ultralytics/yolov5.git\",\n",
    "        str(YOLOV5_DIR)\n",
    "    ])\n",
    "    print(f\"âœ“ YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³å®Œäº†\")\n",
    "\n",
    "# YOLOv5ã®ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "print(f\"\\nğŸ“¦ YOLOv5ã®ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "requirements_file = YOLOV5_DIR / \"requirements.txt\"\n",
    "if requirements_file.exists():\n",
    "    subprocess.check_call([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \n",
    "        \"-r\", str(requirements_file)\n",
    "    ])\n",
    "    print(f\"âœ“ YOLOv5ã®ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
    "else:\n",
    "    print(f\"âš ï¸  è­¦å‘Š: requirements.txtãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "# AWS CLIï¼ˆS3ã‚¢ã‚¯ã‚»ã‚¹ç”¨ï¼‰\n",
    "try:\n",
    "    result = subprocess.run([\"aws\", \"--version\"], capture_output=True, text=True)\n",
    "    print(f\"\\nâœ“ AWS CLI ã¯æ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿: {result.stdout.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nğŸ“¦ AWS CLI ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "    subprocess.check_call([\"pip\", \"install\", \"-q\", \"awscli\"])\n",
    "    print(\"âœ“ AWS CLI ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
    "\n",
    "print(f\"\\nâœ“ YOLOv5ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†\")\n",
    "print(f\"  YOLOv5ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {YOLOV5_DIR}\")\n",
    "print(f\"  train.py: {YOLOV5_DIR / 'train.py'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5216b940",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«3: S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å±•é–‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å±•é–‹\n",
    "# ============================================\n",
    "\n",
    "import subprocess\n",
    "import tarfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å±•é–‹\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "if CLEAN_LOCAL_DATA and LOCAL_DATA_DIR.exists():\n",
    "    print(f\"ğŸ—‘ï¸  æ—¢å­˜ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤: {LOCAL_DATA_DIR}\")\n",
    "    shutil.rmtree(LOCAL_DATA_DIR)\n",
    "\n",
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "download_dir = LOCAL_DATA_DIR.parent / \"yolo_tars_download\"\n",
    "download_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹tarãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ\n",
    "tar_files = [\n",
    "    \"train_images.tar\",\n",
    "    \"train_labels.tar\",\n",
    "    \"val_images.tar\",\n",
    "    \"val_labels.tar\",\n",
    "    \"meta.tar\"\n",
    "]\n",
    "\n",
    "# S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "print(f\"\\nğŸ“¥ S3ã‹ã‚‰tarãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "print(f\"   S3ãƒ‘ã‚¹: s3://{S3_BUCKET}/{S3_PREFIX}/\")\n",
    "\n",
    "downloaded_files = []\n",
    "for tar_name in tar_files:\n",
    "    s3_path = f\"s3://{S3_BUCKET}/{S3_PREFIX}/{tar_name}\"\n",
    "    local_tar_path = download_dir / tar_name\n",
    "    \n",
    "    if local_tar_path.exists():\n",
    "        size_mb = local_tar_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"\\nâœ“ {tar_name} ã¯æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {size_mb:.2f} MB\")\n",
    "        downloaded_files.append(local_tar_path)\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nğŸ“¥ {tar_name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"aws\", \"s3\", \"cp\", s3_path, str(local_tar_path)],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        size_mb = local_tar_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  âœ“ å®Œäº†: {size_mb:.2f} MB\")\n",
    "        downloaded_files.append(local_tar_path)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"  âŒ å¤±æ•—: {e.stderr}\")\n",
    "        raise\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™\n",
    "LOCAL_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "train_images_dir = LOCAL_DATA_DIR / \"train\" / \"images\"\n",
    "train_labels_dir = LOCAL_DATA_DIR / \"train\" / \"labels\"\n",
    "val_images_dir = LOCAL_DATA_DIR / \"val\" / \"images\"\n",
    "val_labels_dir = LOCAL_DATA_DIR / \"val\" / \"labels\"\n",
    "\n",
    "# tarãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹\n",
    "print(f\"\\nğŸ“¦ tarãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹ä¸­...\")\n",
    "\n",
    "# train_images.tar\n",
    "train_images_tar = download_dir / \"train_images.tar\"\n",
    "if train_images_tar.exists():\n",
    "    if not train_images_dir.exists() or len(list(train_images_dir.glob(\"*\"))) == 0:\n",
    "        train_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"\\nğŸ“¦ train_images.tar ã‚’å±•é–‹ä¸­...\")\n",
    "        with tarfile.open(train_images_tar, 'r') as tar:\n",
    "            tar.extractall(train_images_dir)\n",
    "        print(f\"  âœ“ å®Œäº†: {len(list(train_images_dir.glob('*')))} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "    else:\n",
    "        print(f\"\\nâœ“ train_images ã¯æ—¢ã«å±•é–‹æ¸ˆã¿: {len(list(train_images_dir.glob('*')))} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "\n",
    "# train_labels.tar\n",
    "train_labels_tar = download_dir / \"train_labels.tar\"\n",
    "if train_labels_tar.exists():\n",
    "    if not train_labels_dir.exists() or len(list(train_labels_dir.glob(\"*.txt\"))) == 0:\n",
    "        train_labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"\\nğŸ“¦ train_labels.tar ã‚’å±•é–‹ä¸­...\")\n",
    "        with tarfile.open(train_labels_tar, 'r') as tar:\n",
    "            tar.extractall(train_labels_dir)\n",
    "        print(f\"  âœ“ å®Œäº†: {len(list(train_labels_dir.glob('*.txt')))} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "    else:\n",
    "        print(f\"\\nâœ“ train_labels ã¯æ—¢ã«å±•é–‹æ¸ˆã¿: {len(list(train_labels_dir.glob('*.txt')))} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "\n",
    "# val_images.tar\n",
    "val_images_tar = download_dir / \"val_images.tar\"\n",
    "if val_images_tar.exists():\n",
    "    if not val_images_dir.exists() or len(list(val_images_dir.glob(\"*\"))) == 0:\n",
    "        val_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"\\nğŸ“¦ val_images.tar ã‚’å±•é–‹ä¸­...\")\n",
    "        with tarfile.open(val_images_tar, 'r') as tar:\n",
    "            tar.extractall(val_images_dir)\n",
    "        print(f\"  âœ“ å®Œäº†: {len(list(val_images_dir.glob('*')))} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "    else:\n",
    "        print(f\"\\nâœ“ val_images ã¯æ—¢ã«å±•é–‹æ¸ˆã¿: {len(list(val_images_dir.glob('*')))} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "\n",
    "# val_labels.tar\n",
    "val_labels_tar = download_dir / \"val_labels.tar\"\n",
    "if val_labels_tar.exists():\n",
    "    if not val_labels_dir.exists() or len(list(val_labels_dir.glob(\"*.txt\"))) == 0:\n",
    "        val_labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"\\nğŸ“¦ val_labels.tar ã‚’å±•é–‹ä¸­...\")\n",
    "        with tarfile.open(val_labels_tar, 'r') as tar:\n",
    "            tar.extractall(val_labels_dir)\n",
    "        print(f\"  âœ“ å®Œäº†: {len(list(val_labels_dir.glob('*.txt')))} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "    else:\n",
    "        print(f\"\\nâœ“ val_labels ã¯æ—¢ã«å±•é–‹æ¸ˆã¿: {len(list(val_labels_dir.glob('*.txt')))} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
    "\n",
    "# meta.tarï¼ˆmapping.json, data.yaml, manifest.jsonï¼‰\n",
    "meta_tar = download_dir / \"meta.tar\"\n",
    "if meta_tar.exists():\n",
    "    print(f\"\\nğŸ“¦ meta.tar ã‚’å±•é–‹ä¸­...\")\n",
    "    with tarfile.open(meta_tar, 'r') as tar:\n",
    "        tar.extractall(LOCAL_DATA_DIR)\n",
    "    print(f\"  âœ“ å®Œäº†\")\n",
    "\n",
    "# manifest.jsonã¨data.yamlã®ç¢ºèª\n",
    "manifest_file = LOCAL_DATA_DIR / \"manifest.json\"\n",
    "data_yaml_file = LOCAL_DATA_DIR / \"data.yaml\"\n",
    "\n",
    "if manifest_file.exists():\n",
    "    print(f\"\\nâœ“ manifest.json ã‚’ç¢ºèª: {manifest_file}\")\n",
    "if data_yaml_file.exists():\n",
    "    print(f\"âœ“ data.yaml ã‚’ç¢ºèª: {data_yaml_file}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  è­¦å‘Š: data.yaml ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å¾Œã§ä½œæˆã—ã¾ã™ã€‚\")\n",
    "\n",
    "print(f\"\\nâœ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å±•é–‹å®Œäº†\")\n",
    "print(f\"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹: {LOCAL_DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da3143",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«4: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèª\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèª\n",
    "# ============================================\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç¢ºèª\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã®ç¢ºèª\n",
    "train_images = list(train_images_dir.glob(\"*\"))\n",
    "train_labels = list(train_labels_dir.glob(\"*.txt\"))\n",
    "val_images = list(val_images_dir.glob(\"*\"))\n",
    "val_labels = list(val_labels_dir.glob(\"*.txt\"))\n",
    "\n",
    "print(f\"\\nTrainãƒ‡ãƒ¼ã‚¿:\")\n",
    "print(f\"  ç”»åƒ: {len(train_images)} ä»¶\")\n",
    "print(f\"  ãƒ©ãƒ™ãƒ«: {len(train_labels)} ä»¶\")\n",
    "\n",
    "print(f\"\\nValãƒ‡ãƒ¼ã‚¿:\")\n",
    "print(f\"  ç”»åƒ: {len(val_images)} ä»¶\")\n",
    "print(f\"  ãƒ©ãƒ™ãƒ«: {len(val_labels)} ä»¶\")\n",
    "\n",
    "# 1å¯¾1å¯¾å¿œã®ç¢ºèª\n",
    "train_image_uuids = {f.stem for f in train_images}\n",
    "train_label_uuids = {f.stem for f in train_labels}\n",
    "val_image_uuids = {f.stem for f in val_images}\n",
    "val_label_uuids = {f.stem for f in val_labels}\n",
    "\n",
    "train_matched = train_image_uuids & train_label_uuids\n",
    "val_matched = val_image_uuids & val_label_uuids\n",
    "\n",
    "print(f\"\\n1å¯¾1å¯¾å¿œç¢ºèª:\")\n",
    "print(f\"  Train: {len(train_matched)}/{len(train_images)} ãƒšã‚¢\")\n",
    "print(f\"  Val: {len(val_matched)}/{len(val_images)} ãƒšã‚¢\")\n",
    "\n",
    "# manifest.jsonã®èª­ã¿è¾¼ã¿ï¼ˆã‚¯ãƒ©ã‚¹æƒ…å ±ã‚’å–å¾—ï¼‰\n",
    "if manifest_file.exists():\n",
    "    with open(manifest_file, 'r', encoding='utf-8') as f:\n",
    "        manifest = json.load(f)\n",
    "    \n",
    "    target_classes = manifest.get('target_classes', [])\n",
    "    print(f\"\\nmanifest.jsonã‹ã‚‰å–å¾—ã—ãŸæƒ…å ±:\")\n",
    "    print(f\"  å¯¾è±¡ã‚¯ãƒ©ã‚¹æ•°: {len(target_classes)}\")\n",
    "    print(f\"  ã‚¯ãƒ©ã‚¹: {', '.join(target_classes)}\")\n",
    "    print(f\"  Trainæ•°: {manifest.get('split', {}).get('train_count', 'N/A')}\")\n",
    "    print(f\"  Valæ•°: {manifest.get('split', {}).get('val_count', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  è­¦å‘Š: manifest.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "    target_classes = None\n",
    "\n",
    "print(f\"\\nâœ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç¢ºèªå®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e229db",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«5: data.yamlã®ä½œæˆï¼ˆmanifest.jsonã‹ã‚‰ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# data.yamlã®ä½œæˆï¼ˆmanifest.jsonã‹ã‚‰ï¼‰\n",
    "# ============================================\n",
    "\n",
    "import yaml\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"data.yamlä½œæˆ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# manifest.jsonã‹ã‚‰æƒ…å ±ã‚’å–å¾—\n",
    "if not manifest_file.exists():\n",
    "    raise FileNotFoundError(f\"manifest.jsonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {manifest_file}\")\n",
    "\n",
    "with open(manifest_file, 'r', encoding='utf-8') as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "target_classes = manifest.get('target_classes', [])\n",
    "if not target_classes:\n",
    "    raise ValueError(\"manifest.jsonã«target_classesãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "\n",
    "# data.yamlã‚’ä½œæˆ\n",
    "data_config = {\n",
    "    'path': str(LOCAL_DATA_DIR.absolute()),\n",
    "    'train': 'train/images',\n",
    "    'val': 'val/images',\n",
    "    'nc': len(target_classes),\n",
    "    'names': {idx: cls for idx, cls in enumerate(target_classes)}\n",
    "}\n",
    "\n",
    "# æ—¢å­˜ã®data.yamlãŒã‚ã‚‹å ´åˆã¯ä¸Šæ›¸ã\n",
    "with open(data_yaml_file, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"\\nâœ“ data.yamlã‚’ä½œæˆ: {data_yaml_file}\")\n",
    "print(f\"\\nå†…å®¹:\")\n",
    "print(yaml.dump(data_config, default_flow_style=False, allow_unicode=True))\n",
    "\n",
    "data_path = str(data_yaml_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9560de98",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«6: äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da94f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOLOv5ã®äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã®ç¢ºèª\n",
    "# ============================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"YOLOv5ã®äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã®ç¢ºèª\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# YOLOv5ã®train.pyã¯åˆå›å®Ÿè¡Œæ™‚ã«è‡ªå‹•çš„ã«é‡ã¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™\n",
    "# ã“ã“ã§ã¯è¨­å®šã‚’ç¢ºèªã™ã‚‹ã ã‘ã§ã™\n",
    "\n",
    "print(f\"\\nâœ“ YOLOv5nãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™\")\n",
    "print(f\"  ãƒ¢ãƒ‡ãƒ«å: {MODEL_NAME}\")\n",
    "print(f\"  äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿: {PRETRAINED_WEIGHTS}\")\n",
    "print(f\"  ï¼ˆtrain.pyå®Ÿè¡Œæ™‚ã«è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ï¼‰\")\n",
    "print(f\"\\nâœ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±:\")\n",
    "print(f\"  ã‚¯ãƒ©ã‚¹æ•°: {len(target_classes)} ã‚¯ãƒ©ã‚¹\")\n",
    "print(f\"  ã‚¯ãƒ©ã‚¹: {', '.join(target_classes)}\")\n",
    "print(f\"  data.yaml: {data_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb9ef0",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«7: å­¦ç¿’è¨­å®š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# å­¦ç¿’è¨­å®š\n",
    "# ============================================\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"å­¦ç¿’è¨­å®š\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# å‡ºåŠ›å…ˆã®è¨­å®š\n",
    "LOCAL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# å®Ÿè¡Œåï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_name = f\"{MODEL_NAME}_train_{timestamp}\"\n",
    "\n",
    "print(f\"\\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {data_path}\")\n",
    "print(f\"ã‚¨ãƒãƒƒã‚¯æ•°: {EPOCHS}\")\n",
    "print(f\"ãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE}\")\n",
    "print(f\"ç”»åƒã‚µã‚¤ã‚º: {IMG_SIZE}\")\n",
    "print(f\"ãƒ‡ãƒã‚¤ã‚¹: {'GPU' if DEVICE == 0 else 'CPU'}\")\n",
    "print(f\"å‡ºåŠ›å…ˆ: {LOCAL_OUTPUT_DIR}/{run_name}\")\n",
    "print(f\"\\nã‚¯ãƒ©ã‚¹æ•°: {len(target_classes)} ã‚¯ãƒ©ã‚¹\")\n",
    "print(f\"ã‚¯ãƒ©ã‚¹: {', '.join(target_classes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3818e",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«8: å­¦ç¿’ã®å®Ÿè¡Œ\n",
    "\n",
    "**æ³¨æ„**: ã“ã®ã‚»ãƒ«ã¯é•·æ™‚é–“ã‹ã‹ã‚Šã¾ã™ï¼ˆæ•°æ™‚é–“ï¼‰ã€‚GPUãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bee321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# å­¦ç¿’ã®å®Ÿè¡Œï¼ˆYOLOv5ã®train.pyã‚’ä½¿ç”¨ï¼‰\n",
    "# ============================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"å­¦ç¿’é–‹å§‹ï¼ˆYOLOv5 train.pyï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "# YOLOv5ã®train.pyã®ãƒ‘ã‚¹\n",
    "train_script = YOLOV5_DIR / \"train.py\"\n",
    "\n",
    "if not train_script.exists():\n",
    "    raise FileNotFoundError(f\"train.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {train_script}\")\n",
    "\n",
    "# train.pyã®å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰\n",
    "train_cmd = [\n",
    "    sys.executable,\n",
    "    str(train_script),\n",
    "    \"--data\", str(data_path),\n",
    "    \"--weights\", PRETRAINED_WEIGHTS,  # yolov5n.ptï¼ˆè‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰\n",
    "    \"--epochs\", str(EPOCHS),\n",
    "    \"--batch-size\", str(BATCH_SIZE),\n",
    "    \"--img\", str(IMG_SIZE),\n",
    "    \"--device\", str(DEVICE),\n",
    "    \"--project\", str(LOCAL_OUTPUT_DIR),\n",
    "    \"--name\", run_name,\n",
    "    \"--save-period\", \"10\",  # 10ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ä¿å­˜\n",
    "    \"--val\",  # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡\n",
    "    \"--plots\",  # å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "]\n",
    "\n",
    "print(f\"å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰:\")\n",
    "print(f\"  {' '.join(train_cmd)}\")\n",
    "print()\n",
    "\n",
    "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’YOLOv5ã«å¤‰æ›´ã—ã¦å®Ÿè¡Œ\n",
    "# ï¼ˆYOLOv5ã®train.pyã¯ç›¸å¯¾ãƒ‘ã‚¹ã§ãƒªã‚½ãƒ¼ã‚¹ã‚’å‚ç…§ã™ã‚‹ãŸã‚ï¼‰\n",
    "original_cwd = Path.cwd()\n",
    "try:\n",
    "    os.chdir(YOLOV5_DIR)\n",
    "    print(f\"ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {YOLOV5_DIR}\")\n",
    "    print()\n",
    "    \n",
    "    # å­¦ç¿’å®Ÿè¡Œ\n",
    "    result = subprocess.run(\n",
    "        train_cmd,\n",
    "        check=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"å­¦ç¿’å®Œäº†\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"çµ‚äº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"\\nâŒ ã‚¨ãƒ©ãƒ¼: å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ\")\n",
    "    print(f\"   çµ‚äº†ã‚³ãƒ¼ãƒ‰: {e.returncode}\")\n",
    "    raise\n",
    "finally:\n",
    "    os.chdir(original_cwd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf74ce64",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«9: å­¦ç¿’çµæœã®ç¢ºèª\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd2c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# å­¦ç¿’çµæœã®ç¢ºèª\n",
    "# ============================================\n",
    "\n",
    "results_dir = LOCAL_OUTPUT_DIR / run_name\n",
    "weights_dir = results_dir / \"weights\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"å­¦ç¿’çµæœ\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {results_dir}\")\n",
    "\n",
    "# é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "if weights_dir.exists():\n",
    "    weight_files = list(weights_dir.glob('*.pt'))\n",
    "    print(f\"\\né‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(weight_files)} ä»¶\")\n",
    "    \n",
    "    for wf in sorted(weight_files):\n",
    "        size_mb = wf.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {wf.name}: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«\n",
    "    best_model = weights_dir / \"best.pt\"\n",
    "    if best_model.exists():\n",
    "        print(f\"\\nâœ“ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model.name}\")\n",
    "        print(f\"  ã‚µã‚¤ã‚º: {best_model.stat().st_size / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«\n",
    "    last_model = weights_dir / \"last.pt\"\n",
    "    if last_model.exists():\n",
    "        print(f\"âœ“ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«: {last_model.name}\")\n",
    "        print(f\"  ã‚µã‚¤ã‚º: {last_model.stat().st_size / (1024 * 1024):.2f} MB\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "# å­¦ç¿’æ›²ç·šã®ç¢ºèª\n",
    "results_csv = results_dir / \"results.csv\"\n",
    "if results_csv.exists():\n",
    "    print(f\"\\nå­¦ç¿’æ›²ç·šãƒ‡ãƒ¼ã‚¿: {results_csv}\")\n",
    "\n",
    "# ãƒ—ãƒ­ãƒƒãƒˆç”»åƒã®ç¢ºèª\n",
    "plot_files = list(results_dir.glob(\"*.png\"))\n",
    "if len(plot_files) > 0:\n",
    "    print(f\"\\nãƒ—ãƒ­ãƒƒãƒˆç”»åƒæ•°: {len(plot_files)} ä»¶\")\n",
    "    for pf in plot_files:\n",
    "        print(f\"  {pf.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98f8ff",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«10: ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "# ============================================\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# AWSèªè¨¼æƒ…å ±ã®ç¢ºèª\n",
    "try:\n",
    "    result = subprocess.run([\"aws\", \"sts\", \"get-caller-identity\"], \n",
    "                          capture_output=True, text=True, check=True)\n",
    "    print(f\"\\nâœ“ AWSèªè¨¼æƒ…å ±ã‚’ç¢ºèª\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"\\nâŒ ã‚¨ãƒ©ãƒ¼: AWSèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    print(\"   ã‚»ãƒ«0ã§AWSèªè¨¼æƒ…å ±ã‚’è¨­å®šã—ã¦ãã ã•ã„\")\n",
    "    raise\n",
    "\n",
    "# S3ã®ä¿å­˜å…ˆãƒ‘ã‚¹\n",
    "s3_weights_prefix = f\"{S3_PREFIX}/weights/{run_name}\"\n",
    "\n",
    "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ\n",
    "files_to_upload = []\n",
    "\n",
    "# 1. æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ï¼ˆbest.ptï¼‰\n",
    "best_model = weights_dir / \"best.pt\"\n",
    "if best_model.exists():\n",
    "    files_to_upload.append((\"æœ€è‰¯ãƒ¢ãƒ‡ãƒ«\", best_model, f\"{s3_weights_prefix}/best.pt\"))\n",
    "else:\n",
    "    print(\"âš ï¸  æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ï¼ˆbest.ptï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "# 2. æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ï¼ˆlast.ptï¼‰\n",
    "last_model = weights_dir / \"last.pt\"\n",
    "if last_model.exists():\n",
    "    files_to_upload.append((\"æœ€çµ‚ãƒ¢ãƒ‡ãƒ«\", last_model, f\"{s3_weights_prefix}/last.pt\"))\n",
    "\n",
    "# 3. å­¦ç¿’æ›²ç·šãƒ‡ãƒ¼ã‚¿ï¼ˆresults.csvï¼‰\n",
    "results_csv = results_dir / \"results.csv\"\n",
    "if results_csv.exists():\n",
    "    files_to_upload.append((\"å­¦ç¿’æ›²ç·šãƒ‡ãƒ¼ã‚¿\", results_csv, f\"{s3_weights_prefix}/results.csv\"))\n",
    "\n",
    "# 4. data.yamlï¼ˆå­¦ç¿’ã«ä½¿ç”¨ã—ãŸè¨­å®šï¼‰\n",
    "if data_yaml_file.exists():\n",
    "    files_to_upload.append((\"data.yaml\", data_yaml_file, f\"{s3_weights_prefix}/data.yaml\"))\n",
    "\n",
    "# 5. manifest.jsonï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ï¼‰\n",
    "if manifest_file.exists():\n",
    "    files_to_upload.append((\"manifest.json\", manifest_file, f\"{s3_weights_prefix}/manifest.json\"))\n",
    "\n",
    "print(f\"\\nã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files_to_upload)} ä»¶\")\n",
    "print(f\"S3ãƒ‘ã‚¹: s3://{S3_BUCKET}/{s3_weights_prefix}/\")\n",
    "print()\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "uploaded = []\n",
    "failed = []\n",
    "\n",
    "for desc, local_file, s3_key in files_to_upload:\n",
    "    if not local_file.exists():\n",
    "        print(f\"âš ï¸  ã‚¹ã‚­ãƒƒãƒ—: {desc}ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼‰\")\n",
    "        continue\n",
    "    \n",
    "    s3_path = f\"s3://{S3_BUCKET}/{s3_key}\"\n",
    "    size_mb = local_file.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"\\nğŸ“¤ {desc} ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "    print(f\"   ãƒ•ã‚¡ã‚¤ãƒ«: {local_file.name}\")\n",
    "    print(f\"   ã‚µã‚¤ã‚º: {size_mb:.2f} MB\")\n",
    "    print(f\"   S3ãƒ‘ã‚¹: {s3_path}\")\n",
    "    \n",
    "    max_retries = 3\n",
    "    retry_count = 0\n",
    "    success = False\n",
    "    \n",
    "    while retry_count < max_retries and not success:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = subprocess.run(\n",
    "                [\"aws\", \"s3\", \"cp\", str(local_file), s3_path],\n",
    "                capture_output=True, text=True, check=True\n",
    "            )\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"  âœ“ å®Œäº†: {elapsed:.1f}ç§’\")\n",
    "            uploaded.append(desc)\n",
    "            success = True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            retry_count += 1\n",
    "            if retry_count < max_retries:\n",
    "                print(f\"  âš ï¸  ãƒªãƒˆãƒ©ã‚¤ {retry_count}/{max_retries}...\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(f\"  âŒ å¤±æ•—: {e.stderr}\")\n",
    "                failed.append(desc)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  æˆåŠŸ: {len(uploaded)} ä»¶\")\n",
    "if uploaded:\n",
    "    for name in uploaded:\n",
    "        print(f\"    âœ“ {name}\")\n",
    "\n",
    "print(f\"  å¤±æ•—: {len(failed)} ä»¶\")\n",
    "if failed:\n",
    "    for name in failed:\n",
    "        print(f\"    âŒ {name}\")\n",
    "\n",
    "if len(uploaded) == len(files_to_upload):\n",
    "    print(f\"\\nâœ“ å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "    print(f\"  S3ãƒ‘ã‚¹: s3://{S3_BUCKET}/{s3_weights_prefix}/\")\n",
    "    print(f\"\\næ¬¡å›ã¯ä»¥ä¸‹ã®ãƒ‘ã‚¹ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚ã¾ã™:\")\n",
    "    print(f\"  æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: s3://{S3_BUCKET}/{s3_weights_prefix}/best.pt\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  ä¸€éƒ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "    print(f\"  å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯æ‰‹å‹•ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
