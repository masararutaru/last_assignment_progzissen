{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b01f20",
   "metadata": {},
   "source": [
    "# Rawãƒ‡ãƒ¼ã‚¿ã‹ã‚‰YOLOå½¢å¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆï¼†S3ãƒ‘ãƒƒã‚­ãƒ³ã‚°\n",
    "\n",
    "## ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ç›®çš„\n",
    "\n",
    "Kaggleã‹ã‚‰å–å¾—ã—ãŸrawãƒ‡ãƒ¼ã‚¿ï¼ˆzipï¼‰ã‚’ `/content/tmp` ä¸Šã§å±•é–‹ã—ã€\n",
    "rawãƒ©ãƒ™ãƒ«ï¼ˆLaTeXï¼‹åº§æ¨™ï¼‰ã¨ç”»åƒã‚’çªåˆã—ã¦ã€\n",
    "æŒ‡å®šã—ãŸã‚¯ãƒ©ã‚¹ï¼ˆä¾‹ï¼š1ã€œ9ã€+,-,*,/,=,(,),x ãªã©ï¼‰ã®ã¿æŠ½å‡ºã—ã€\n",
    "YOLOå­¦ç¿’ç”¨ã«ä»¥ä¸‹ã®æ§‹é€ ã‚’ç”Ÿæˆã—ã¾ã™ï¼š\n",
    "\n",
    "```\n",
    "out/\n",
    "  train/images\n",
    "  train/labels\n",
    "  val/images\n",
    "  val/labels\n",
    "  mapping.json\n",
    "  data.yaml\n",
    "  manifest.json\n",
    "```\n",
    "\n",
    "æœ€å¾Œã« tar ã«ã¾ã¨ã‚ã¦S3ã¸ã‚¢ãƒƒãƒ—ã§ãã‚‹çŠ¶æ…‹ã«ã—ã¾ã™ã€‚\n",
    "\n",
    "## å…¥åŠ›\n",
    "\n",
    "- Kaggleã‹ã‚‰è½ã¨ã—ãŸ raw.zipï¼ˆã¾ãŸã¯å±•é–‹æ¸ˆã¿ãƒ•ã‚©ãƒ«ãƒ€ï¼‰\n",
    "- rawç”»åƒãƒ•ã‚©ãƒ«ãƒ€ï¼ˆä¾‹ï¼šraw/images/... ã¾ãŸã¯ batch*/background_images/...ï¼‰\n",
    "- rawãƒ©ãƒ™ãƒ«ï¼ˆJSONå½¢å¼ã€LaTeXæ–‡å­—åˆ—ï¼‹bboxï¼‰\n",
    "- æ—¢å­˜ã® mapping.json ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚‚å…¥åŠ›ã¨ã—ã¦ä½¿ãˆã‚‹ï¼ˆå¿…é ˆã§ã¯ãªã„ï¼‰\n",
    "\n",
    "## å‡ºåŠ›\n",
    "\n",
    "### YOLOãƒ‡ãƒ¼ã‚¿ï¼ˆå±•é–‹çŠ¶æ…‹ï¼‰\n",
    "`/content/tmp/yolo_out/` é…ä¸‹ã«ä»¥ä¸‹ã‚’ç”Ÿæˆï¼š\n",
    "- `train/images/`, `train/labels/`\n",
    "- `val/images/`, `val/labels/`\n",
    "- `mapping.json`, `data.yaml`, `manifest.json`\n",
    "\n",
    "### tarã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆS3ä¿ç®¡ç”¨ï¼‰\n",
    "- `train_images.tar`, `train_labels.tar`\n",
    "- `val_images.tar`, `val_labels.tar`\n",
    "- `meta.tar`ï¼ˆmapping.json, data.yaml, manifest.json ã‚’ã¾ã¨ã‚ã‚‹ï¼‰\n",
    "\n",
    "## å®Ÿè¡Œé †åº\n",
    "\n",
    "**ä¸Šã‹ã‚‰é †ã«å…¨ã¦ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚**\n",
    "\n",
    "å„ã‚»ãƒ«ã¯å†ªç­‰æ€§ï¼ˆidempotentï¼‰ã‚’ä¿è¨¼ã—ã¦ãŠã‚Šã€\n",
    "æ—¢ã«å‡¦ç†æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660922e0",
   "metadata": {},
   "source": [
    "##ã‚»ãƒ«0: awsã®èªè¨¼\n",
    "ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®\"AWS_ACCESS_KEY_ID\"ã¨\"AWS_SECRET_ACCESS_KEY\"ã‚’åŸ‹ã‚ã¦å®Ÿè¡Œã—ã¦ã‹ã‚‰ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAxxxxxxxxxxxxxxxx\"#åŸ‹ã‚ã¦ã­ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"xxxxxxxxxxxxxxxxxxxx\"#åŸ‹ã‚ã¦ã­ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"ap-northeast-1\"\n",
    "\n",
    "print(\"AWS env set\")\n",
    "\n",
    "assert \"AWS_ACCESS_KEY_ID\" in os.environ, \"AWS_ACCESS_KEY_ID not set\"\n",
    "assert \"AWS_SECRET_ACCESS_KEY\" in os.environ, \"AWS_SECRET_ACCESS_KEY not set\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ffe0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = 'masapirika'\n",
    "os.environ['KAGGLE_KEY'] = '================'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0763f37",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«1: è¨­å®šï¼ˆã“ã“ã ã‘è§¦ã‚Œã°OKï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ee0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# è¨­å®šï¼ˆã“ã“ã ã‘å¤‰æ›´ã™ã‚Œã°OKï¼‰\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== ãƒ‘ã‚¹è¨­å®š =====\n",
    "# Kaggleã‹ã‚‰è½ã¨ã—ãŸraw.zipã®ãƒ‘ã‚¹ï¼ˆæ—¢ã«DLæ¸ˆã¿ãªã‚‰ãã®ãƒ‘ã‚¹ã€æœªDLãªã‚‰Noneï¼‰\n",
    "RAW_ZIP_PATH = None  # ä¾‹: \"/content/raw.zip\" ã¾ãŸã¯ Noneï¼ˆã‚»ãƒ«3ã§DLï¼‰\n",
    "\n",
    "# rawãƒ‡ãƒ¼ã‚¿ã®å±•é–‹å…ˆï¼ˆ/content/tmp/rawï¼‰\n",
    "RAW_DIR = Path(\"/content/tmp/raw\")\n",
    "\n",
    "# YOLOãƒ‡ãƒ¼ã‚¿ã®å‡ºåŠ›å…ˆ\n",
    "OUT_DIR = Path(\"/content/tmp/yolo_out\")\n",
    "\n",
    "# ===== train/valåˆ†å‰²è¨­å®š =====\n",
    "SEED = 42  # å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰\n",
    "VAL_RATIO = 0.2  # 20%ã‚’valã«\n",
    "\n",
    "# ===== æŠ½å‡ºå¯¾è±¡ã‚¯ãƒ©ã‚¹ä¸€è¦§ =====\n",
    "# æ‹¡å¼µç‰ˆï¼šç´„61ã‚¯ãƒ©ã‚¹ï¼ˆKaggle Aida Calculus Math Handwriting Recognition Datasetå¯¾å¿œï¼‰\n",
    "# ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹å ´åˆã¯ã“ã“ã‚’å¤‰æ›´\n",
    "TARGET_CLASSES = [\n",
    "    # æ•°å­—ï¼ˆ10ã‚¯ãƒ©ã‚¹ï¼‰\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "    # åŸºæœ¬æ¼”ç®—å­ï¼ˆ5ã‚¯ãƒ©ã‚¹ï¼‰\n",
    "    '+', '-', '*', '/', '=',\n",
    "    # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆå¤‰æ•°ï¼ˆå°æ–‡å­—26ã‚¯ãƒ©ã‚¹ï¼‰\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "    # æ‹¬å¼§é¡ï¼ˆ3ã‚¯ãƒ©ã‚¹ï¼‰\n",
    "    '(', ')', '|',  # ä¸¸æ‹¬å¼§2ç¨® + çµ¶å¯¾å€¤è¨˜å·\n",
    "    # ã‚®ãƒªã‚·ãƒ£æ–‡å­—ãƒ»å®šæ•°ï¼ˆ3ã‚¯ãƒ©ã‚¹ï¼‰\n",
    "    'Ï€',  # å††å‘¨ç‡\n",
    "    'Î¸',  # è§’åº¦å¤‰æ•°\n",
    "    'âˆ',  # ç„¡é™å¤§\n",
    "    # æ³¨æ„: 'e'ï¼ˆè‡ªç„¶å¯¾æ•°ã®åº•ï¼‰ã¯ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆå¤‰æ•°ã®'e'ã¨é‡è¤‡ã™ã‚‹ãŸã‚å‰Šé™¤\n",
    "    # ç‰¹æ®Šè¨˜å·ï¼ˆ2ã‚¯ãƒ©ã‚¹ï¼‰\n",
    "    'â†’',  # çŸ¢å°ï¼ˆãƒªãƒŸãƒƒãƒˆã®ã€Œï½ã«è¿‘ã¥ãã€ï¼‰\n",
    "    'âˆš',  # ãƒ«ãƒ¼ãƒˆè¨˜å·ï¼ˆå¹³æ–¹æ ¹ï¼‰\n",
    "    # é–¢æ•°åï¼ˆ12ã‚¯ãƒ©ã‚¹ï¼‰\n",
    "    'sin',  # æ­£å¼¦\n",
    "    'cos',  # ä½™å¼¦\n",
    "    'tan',  # æ­£æ¥\n",
    "    'sec',  # æ­£å‰²\n",
    "    'csc',  # ä½™å‰²\n",
    "    'cot',  # ä½™æ¥\n",
    "    'ln',   # è‡ªç„¶å¯¾æ•°\n",
    "    'log',  # å¯¾æ•°\n",
    "    'exp',  # æŒ‡æ•°é–¢æ•°\n",
    "    'sqrt', # å¹³æ–¹æ ¹\n",
    "    'lim',  # æ¥µé™\n",
    "    'abs',  # çµ¶å¯¾å€¤\n",
    "]\n",
    "\n",
    "# ===== S3è¨­å®š =====\n",
    "S3_BUCKET = \"km62m-ml-storage\"  # S3ãƒã‚±ãƒƒãƒˆå\n",
    "S3_PREFIX = \"yolo-dataset/v1\"  # S3ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹\n",
    "UPLOAD_TO_S3 = True  # Trueã«ã™ã‚‹ã¨S3ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "# ===== Kaggleè¨­å®šï¼ˆã‚»ãƒ«3ã§ä½¿ç”¨ï¼‰ =====\n",
    "KAGGLE_DATASET = \"aidapearson/ocr-data\"  # Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå\n",
    "KAGGLE_DOWNLOAD_DIR = Path(\"/content/tmp\")\n",
    "\n",
    "# ===== ãã®ä»– =====\n",
    "# æ—¢å­˜ã®OUT_DIRã‚’å‰Šé™¤ã™ã‚‹ã‹ï¼ˆTrue: å‰Šé™¤ã€False: ä¸Šæ›¸ãï¼‰\n",
    "CLEAN_OUT_DIR = False  # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ãŸã„å ´åˆã¯False\n",
    "\n",
    "print(\"âœ“ è¨­å®šå®Œäº†\")\n",
    "print(f\"  RAW_DIR: {RAW_DIR}\")\n",
    "print(f\"  OUT_DIR: {OUT_DIR}\")\n",
    "print(f\"  VAL_RATIO: {VAL_RATIO}\")\n",
    "print(f\"  å¯¾è±¡ã‚¯ãƒ©ã‚¹æ•°: {len(TARGET_CLASSES)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c64d88",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«5.5: ä»»æ„å®Ÿè¡Œ - ç”Ÿãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ç¢ºèªï¼ˆlimã€absã€expã€sqrtãªã©ï¼‰\n",
    "\n",
    "ã“ã®ã‚»ãƒ«ã¯ä»»æ„å®Ÿè¡Œã§ã™ã€‚limã€absã€expã€sqrtãªã©ã®é–¢æ•°åãŒå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§ã©ã®ã‚ˆã†ã«è¡¨ç¾ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã§ãã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f6768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ä»»æ„å®Ÿè¡Œ: ç”Ÿãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ç¢ºèªï¼ˆlimã€absã€expã€sqrtãªã©ï¼‰\n",
    "# ============================================\n",
    "# ã“ã®ã‚»ãƒ«ã¯ä»»æ„å®Ÿè¡Œã§ã™ã€‚å¿…è¦ã«å¿œã˜ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# ç¢ºèªå¯¾è±¡ã®é–¢æ•°å\n",
    "target_functions = ['lim', 'abs', 'exp', 'sqrt']\n",
    "\n",
    "# é–¢æ•°åã®å‡ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨˜éŒ²\n",
    "function_patterns = defaultdict(list)\n",
    "latex_command_patterns = defaultdict(list)\n",
    "\n",
    "# ç¢ºèªã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰\n",
    "max_samples_to_check = 1000\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"é–¢æ•°åã®å‡ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºèª\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ç¢ºèªã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°: {max_samples_to_check}\")\n",
    "print()\n",
    "\n",
    "sample_count = 0\n",
    "for uuid, sample in samples.items():\n",
    "    if sample_count >= max_samples_to_check:\n",
    "        break\n",
    "    \n",
    "    latex_chars = sample.get('latex_chars', [])\n",
    "    \n",
    "    # 1. é€£ç¶šã™ã‚‹æ–‡å­—åˆ—ã¨ã—ã¦é–¢æ•°åã‚’æ¢ã™ï¼ˆ'l', 'i', 'm' â†’ 'lim'ï¼‰\n",
    "    for func in target_functions:\n",
    "        for i in range(len(latex_chars) - len(func) + 1):\n",
    "            candidate = ''.join(latex_chars[i:i+len(func)])\n",
    "            if candidate == func:\n",
    "                context = latex_chars[max(0, i-3):min(len(latex_chars), i+len(func)+3)]\n",
    "                function_patterns[func].append({\n",
    "                    'context': context,\n",
    "                    'uuid': uuid,\n",
    "                    'type': 'concatenated'\n",
    "                })\n",
    "    \n",
    "    # 2. LaTeXã‚³ãƒãƒ³ãƒ‰å½¢å¼ã‚’æ¢ã™ï¼ˆ'\\lim_', '\\sqrt'ãªã©ï¼‰\n",
    "    for func in target_functions:\n",
    "        # LaTeXã‚³ãƒãƒ³ãƒ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "        latex_patterns = [\n",
    "            f'\\\\{func}',      # '\\lim', '\\sqrt'\n",
    "            f'\\\\{func}_',     # '\\lim_', '\\sqrt_'\n",
    "            f'\\\\{func}{{',    # '\\lim{', '\\sqrt{'\n",
    "        ]\n",
    "        \n",
    "        for char in latex_chars:\n",
    "            for pattern in latex_patterns:\n",
    "                if pattern in char:\n",
    "                    latex_command_patterns[func].append({\n",
    "                        'char': char,\n",
    "                        'uuid': uuid,\n",
    "                        'type': 'latex_command'\n",
    "                    })\n",
    "                    break\n",
    "    \n",
    "    sample_count += 1\n",
    "\n",
    "# çµæœã‚’è¡¨ç¤º\n",
    "print(f\"ç¢ºèªã—ãŸã‚µãƒ³ãƒ—ãƒ«æ•°: {sample_count}\")\n",
    "print()\n",
    "\n",
    "for func in target_functions:\n",
    "    print(f\"\\n{func}:\")\n",
    "    \n",
    "    # é€£ç¶šæ–‡å­—åˆ—ã¨ã—ã¦ã®å‡ºç¾\n",
    "    if func in function_patterns:\n",
    "        print(f\"  é€£ç¶šæ–‡å­—åˆ—ã¨ã—ã¦ã®å‡ºç¾: {len(function_patterns[func])} å›\")\n",
    "        if len(function_patterns[func]) > 0:\n",
    "            print(\"  ä¾‹ï¼ˆæœ€åˆã®3ä»¶ï¼‰:\")\n",
    "            for i, pattern in enumerate(function_patterns[func][:3]):\n",
    "                print(f\"    {i+1}. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {pattern['context']}\")\n",
    "    else:\n",
    "        print(f\"  é€£ç¶šæ–‡å­—åˆ—ã¨ã—ã¦ã®å‡ºç¾: 0 å›\")\n",
    "    \n",
    "    # LaTeXã‚³ãƒãƒ³ãƒ‰å½¢å¼ã§ã®å‡ºç¾\n",
    "    if func in latex_command_patterns:\n",
    "        print(f\"  LaTeXã‚³ãƒãƒ³ãƒ‰å½¢å¼ã§ã®å‡ºç¾: {len(latex_command_patterns[func])} å›\")\n",
    "        if len(latex_command_patterns[func]) > 0:\n",
    "            print(\"  ä¾‹ï¼ˆæœ€åˆã®5ä»¶ï¼‰:\")\n",
    "            unique_chars = set()\n",
    "            for pattern in latex_command_patterns[func][:10]:\n",
    "                unique_chars.add(pattern['char'])\n",
    "            for i, char in enumerate(list(unique_chars)[:5]):\n",
    "                print(f\"    {i+1}. '{char}'\")\n",
    "    else:\n",
    "        print(f\"  LaTeXã‚³ãƒãƒ³ãƒ‰å½¢å¼ã§ã®å‡ºç¾: 0 å›\")\n",
    "    \n",
    "    if func not in function_patterns and func not in latex_command_patterns:\n",
    "        print(f\"  â†’ ã“ã®é–¢æ•°åã¯ãƒ‡ãƒ¼ã‚¿å†…ã§è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n",
    "\n",
    "# ã™ã¹ã¦ã®visible_charsã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ï¼ˆé–¢æ•°åé–¢é€£ï¼‰ã‚’ç¢ºèª\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"visible_charsã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ï¼ˆé–¢æ•°åé–¢é€£ãƒ»é•·ã•2æ–‡å­—ä»¥ä¸Šï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "unique_chars = set()\n",
    "for uuid, sample in list(samples.items())[:max_samples_to_check]:\n",
    "    latex_chars = sample.get('latex_chars', [])\n",
    "    for char in latex_chars:\n",
    "        if len(char) > 1 or any(func in char for func in target_functions):\n",
    "            unique_chars.add(char)\n",
    "\n",
    "print(f\"ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ã®æ•°: {len(unique_chars)}\")\n",
    "if len(unique_chars) > 0:\n",
    "    print(\"å€¤ã®ä¾‹ï¼ˆæœ€åˆã®30ä»¶ï¼‰:\")\n",
    "    for char in sorted(unique_chars)[:30]:\n",
    "        print(f\"  '{char}'\")\n",
    "else:\n",
    "    print(\"  é•·ã•2æ–‡å­—ä»¥ä¸Šã®å€¤ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n",
    "\n",
    "print(\"\\nâœ“ ç¢ºèªå®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44074697",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«5.6: ä»»æ„å®Ÿè¡Œ - JSONãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ç¢ºèª\n",
    "\n",
    "ã“ã®ã‚»ãƒ«ã¯ä»»æ„å®Ÿè¡Œã§ã™ã€‚JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å®Ÿéš›ã®æ§‹é€ ã‚’ç¢ºèªã§ãã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d6591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ä»»æ„å®Ÿè¡Œ: JSONãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ç¢ºèª\n",
    "# ============================================\n",
    "# ã“ã®ã‚»ãƒ«ã¯ä»»æ„å®Ÿè¡Œã§ã™ã€‚JSONãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹é€ ã‚’ç¢ºèªã§ãã¾ã™ã€‚\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢\n",
    "json_files = list(RAW_DIR.rglob(\"kaggle_data_*.json\"))\n",
    "if len(json_files) == 0:\n",
    "    print(\"âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚»ãƒ«4ï¼ˆunzipï¼‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "else:\n",
    "    print(f\"è¦‹ã¤ã‹ã£ãŸJSONãƒ•ã‚¡ã‚¤ãƒ«: {len(json_files)} ä»¶\")\n",
    "    print(f\"æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª: {json_files[0]}\")\n",
    "    print()\n",
    "    \n",
    "    # æœ€åˆã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(json_files[0], 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, list) and len(data) > 0:\n",
    "        sample = data[0]\n",
    "        print(\"=\" * 80)\n",
    "        print(\"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ï¼ˆæœ€åˆã®1ä»¶ï¼‰\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"UUID: {sample.get('uuid', 'N/A')}\")\n",
    "        print(f\"LaTeX: {sample.get('latex', 'N/A')}\")\n",
    "        print()\n",
    "        \n",
    "        if 'image_data' in sample:\n",
    "            img_data = sample['image_data']\n",
    "            print(\"image_dataã®æ§‹é€ :\")\n",
    "            print(f\"  width: {img_data.get('width', 'N/A')}\")\n",
    "            print(f\"  height: {img_data.get('height', 'N/A')}\")\n",
    "            \n",
    "            visible_chars = img_data.get('visible_latex_chars', [])\n",
    "            full_chars = img_data.get('full_latex_chars', [])\n",
    "            \n",
    "            print(f\"\\n  visible_latex_charsã®æ•°: {len(visible_chars)}\")\n",
    "            print(f\"  full_latex_charsã®æ•°: {len(full_chars)}\")\n",
    "            \n",
    "            print(f\"\\n  visible_latex_charsï¼ˆæœ€åˆã®20ä»¶ï¼‰:\")\n",
    "            for i, char in enumerate(visible_chars[:20]):\n",
    "                print(f\"    [{i}] '{char}'\")\n",
    "            \n",
    "            if len(visible_chars) > 20:\n",
    "                print(f\"    ... ä»– {len(visible_chars) - 20} ä»¶\")\n",
    "            \n",
    "            print(f\"\\n  full_latex_charsï¼ˆæœ€åˆã®20ä»¶ï¼‰:\")\n",
    "            for i, char in enumerate(full_chars[:20]):\n",
    "                print(f\"    [{i}] '{char}'\")\n",
    "            \n",
    "            if len(full_chars) > 20:\n",
    "                print(f\"    ... ä»– {len(full_chars) - 20} ä»¶\")\n",
    "            \n",
    "            # limã€absã€expã€sqrtã‚’å«ã‚€æ–‡å­—ã‚’æ¢ã™\n",
    "            print(f\"\\n  lim/abs/exp/sqrtã‚’å«ã‚€æ–‡å­—:\")\n",
    "            target_patterns = ['lim', 'abs', 'exp', 'sqrt']\n",
    "            found_chars = []\n",
    "            for char in visible_chars:\n",
    "                for pattern in target_patterns:\n",
    "                    if pattern in char or f'\\\\{pattern}' in char:\n",
    "                        found_chars.append(char)\n",
    "                        break\n",
    "            \n",
    "            if found_chars:\n",
    "                unique_found = list(set(found_chars))\n",
    "                for char in unique_found[:10]:\n",
    "                    print(f\"    '{char}'\")\n",
    "                if len(unique_found) > 10:\n",
    "                    print(f\"    ... ä»– {len(unique_found) - 10} ä»¶\")\n",
    "            else:\n",
    "                print(\"    è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n",
    "    else:\n",
    "        print(\"âŒ ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚\")\n",
    "    \n",
    "    print(\"\\nâœ“ ç¢ºèªå®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4ea723",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«5.7: ä»»æ„å®Ÿè¡Œ - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®è¨˜å·ã®ç¶²ç¾…æ€§ç¢ºèª\n",
    "\n",
    "ã“ã®ã‚»ãƒ«ã¯ä»»æ„å®Ÿè¡Œã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ã™ã¹ã¦ã®è¨˜å·ãŒèªè­˜ã§ãã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã§ãã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00036248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ä»»æ„å®Ÿè¡Œ: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®è¨˜å·ã®ç¶²ç¾…æ€§ç¢ºèª\n",
    "# ============================================\n",
    "# ã“ã®ã‚»ãƒ«ã¯ä»»æ„å®Ÿè¡Œã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ã™ã¹ã¦ã®è¨˜å·ãŒèªè­˜ã§ãã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã§ãã¾ã™ã€‚\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# ç¢ºèªã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰\n",
    "max_samples_to_check = 10000\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®è¨˜å·ã®ç¶²ç¾…æ€§ç¢ºèª\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ç¢ºèªã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°: {max_samples_to_check}\")\n",
    "print()\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ã™ã¹ã¦ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªè¨˜å·ã‚’åé›†\n",
    "all_unique_chars = set()\n",
    "sample_count = 0\n",
    "\n",
    "for uuid, sample in samples.items():\n",
    "    if sample_count >= max_samples_to_check:\n",
    "        break\n",
    "    \n",
    "    latex_chars = sample.get('latex_chars', [])\n",
    "    for char in latex_chars:\n",
    "        all_unique_chars.add(char)\n",
    "    \n",
    "    sample_count += 1\n",
    "\n",
    "print(f\"ç¢ºèªã—ãŸã‚µãƒ³ãƒ—ãƒ«æ•°: {sample_count}\")\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªè¨˜å·æ•°: {len(all_unique_chars)}\")\n",
    "print()\n",
    "\n",
    "# å„è¨˜å·ãŒèªè­˜ã§ãã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "recognized_chars = []\n",
    "unrecognized_chars = []\n",
    "recognized_to_class = {}\n",
    "\n",
    "for char in sorted(all_unique_chars):\n",
    "    class_char = latex_to_class(char)\n",
    "    if class_char is not None:\n",
    "        recognized_chars.append(char)\n",
    "        recognized_to_class[char] = class_char\n",
    "    else:\n",
    "        unrecognized_chars.append(char)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"èªè­˜çµæœ\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"èªè­˜ã§ããŸè¨˜å·: {len(recognized_chars)} / {len(all_unique_chars)} ({len(recognized_chars)/len(all_unique_chars)*100:.1f}%)\")\n",
    "print(f\"èªè­˜ã§ããªã‹ã£ãŸè¨˜å·: {len(unrecognized_chars)} / {len(all_unique_chars)} ({len(unrecognized_chars)/len(all_unique_chars)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# èªè­˜ã§ããªã‹ã£ãŸè¨˜å·ã‚’è¡¨ç¤º\n",
    "if len(unrecognized_chars) > 0:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"âš ï¸  èªè­˜ã§ããªã‹ã£ãŸè¨˜å·ï¼ˆè¦ç¢ºèªï¼‰\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # å‡ºç¾é »åº¦ã‚‚ç¢ºèª\n",
    "    char_counts = defaultdict(int)\n",
    "    for uuid, sample in list(samples.items())[:max_samples_to_check]:\n",
    "        latex_chars = sample.get('latex_chars', [])\n",
    "        for char in latex_chars:\n",
    "            if char in unrecognized_chars:\n",
    "                char_counts[char] += 1\n",
    "    \n",
    "    # å‡ºç¾é »åº¦é †ã«ã‚½ãƒ¼ãƒˆ\n",
    "    sorted_unrecognized = sorted(char_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nå‡ºç¾é »åº¦é †ï¼ˆä¸Šä½20ä»¶ï¼‰:\")\n",
    "    for char, count in sorted_unrecognized[:20]:\n",
    "        print(f\"  '{char}': {count} å›\")\n",
    "    \n",
    "    if len(sorted_unrecognized) > 20:\n",
    "        print(f\"  ... ä»– {len(sorted_unrecognized) - 20} ä»¶\")\n",
    "else:\n",
    "    print(\"âœ“ ã™ã¹ã¦ã®è¨˜å·ãŒèªè­˜ã§ãã¦ã„ã¾ã™ï¼\")\n",
    "\n",
    "# èªè­˜ã§ããŸè¨˜å·ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆä¸€éƒ¨è¡¨ç¤ºï¼‰\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"èªè­˜ã§ããŸè¨˜å·ã®ãƒãƒƒãƒ”ãƒ³ã‚°ä¾‹ï¼ˆæœ€åˆã®30ä»¶ï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "for i, char in enumerate(sorted(recognized_chars)[:30]):\n",
    "    print(f\"  '{char}' â†’ '{recognized_to_class[char]}'\")\n",
    "\n",
    "if len(recognized_chars) > 30:\n",
    "    print(f\"  ... ä»– {len(recognized_chars) - 30} ä»¶\")\n",
    "\n",
    "# TARGET_CLASSESã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚‚ç¢ºèª\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TARGET_CLASSESã¨ã®å¯¾å¿œç¢ºèª\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "target_class_set = set(TARGET_CLASSES)\n",
    "mapped_to_target = []\n",
    "mapped_to_non_target = []\n",
    "\n",
    "for char, class_char in recognized_to_class.items():\n",
    "    if class_char in target_class_set:\n",
    "        mapped_to_target.append((char, class_char))\n",
    "    else:\n",
    "        mapped_to_non_target.append((char, class_char))\n",
    "\n",
    "print(f\"TARGET_CLASSESã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸè¨˜å·: {len(mapped_to_target)} ä»¶\")\n",
    "print(f\"TARGET_CLASSESä»¥å¤–ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸè¨˜å·: {len(mapped_to_non_target)} ä»¶\")\n",
    "\n",
    "if len(mapped_to_non_target) > 0:\n",
    "    print(\"\\nâš ï¸  TARGET_CLASSESä»¥å¤–ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸè¨˜å·:\")\n",
    "    for char, class_char in mapped_to_non_target[:10]:\n",
    "        print(f\"  '{char}' â†’ '{class_char}' (TARGET_CLASSESã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“)\")\n",
    "    if len(mapped_to_non_target) > 10:\n",
    "        print(f\"  ... ä»– {len(mapped_to_non_target) - 10} ä»¶\")\n",
    "\n",
    "print(\"\\nâœ“ ç¢ºèªå®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f13290b",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«2: ä¾å­˜ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d4b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# ============================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package, import_name=None):\n",
    "    \"\"\"\n",
    "    ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
    "    package: pipã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å\n",
    "    import_name: importæ™‚ã«ä½¿ç”¨ã™ã‚‹åå‰ï¼ˆNoneã®å ´åˆã¯packageã¨åŒã˜ï¼‰\n",
    "    \"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package\n",
    "    \n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"âœ“ {package} ã¯æ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿\")\n",
    "    except ImportError:\n",
    "        print(f\"ğŸ“¦ {package} ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(f\"âœ“ {package} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
    "\n",
    "# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸\n",
    "# (package_name, import_name) ã®ã‚¿ãƒ—ãƒ«å½¢å¼\n",
    "# import_nameãŒNoneã®å ´åˆã¯package_nameã¨åŒã˜\n",
    "packages = [\n",
    "    (\"kaggle\", None),\n",
    "    (\"pyarrow\", None),\n",
    "    (\"pandas\", None),\n",
    "    (\"tqdm\", None),\n",
    "    (\"Pillow\", \"PIL\"),  # Pillowã¯pipåã ãŒã€importæ™‚ã¯PIL\n",
    "    (\"pyyaml\", None),\n",
    "]\n",
    "\n",
    "for pkg_info in packages:\n",
    "    if isinstance(pkg_info, tuple):\n",
    "        pkg, import_name = pkg_info\n",
    "    else:\n",
    "        pkg, import_name = pkg_info, None\n",
    "    install_package(pkg, import_name)\n",
    "\n",
    "# AWS CLIï¼ˆS3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ï¼‰\n",
    "try:\n",
    "    result = subprocess.run([\"aws\", \"--version\"], capture_output=True, text=True)\n",
    "    print(f\"âœ“ AWS CLI ã¯æ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿: {result.stdout.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ğŸ“¦ AWS CLI ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "    subprocess.check_call([\"pip\", \"install\", \"-q\", \"awscli\"])\n",
    "    print(\"âœ“ AWS CLI ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
    "\n",
    "print(\"\\nâœ“ å…¨ã¦ã®ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®æº–å‚™å®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5fe82e",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«3: Kaggleã‹ã‚‰raw.zipå–å¾—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# Kaggleèªè¨¼ã®ç¢ºèª\n",
    "# =========================\n",
    "has_env = bool(os.getenv(\"KAGGLE_USERNAME\")) and bool(os.getenv(\"KAGGLE_KEY\"))\n",
    "has_json = (Path.home() / \".config\" / \"kaggle\" / \"kaggle.json\").exists() or (Path.home() / \".kaggle\" / \"kaggle.json\").exists()\n",
    "\n",
    "if not (has_env or has_json):\n",
    "    print(\"âš ï¸ Kaggleèªè¨¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "    print(\"  - ç’°å¢ƒå¤‰æ•° KAGGLE_USERNAME / KAGGLE_KEY ã‚’è¨­å®šã™ã‚‹ã‹\")\n",
    "    print(\"  - ~/.config/kaggle/kaggle.json ã¾ãŸã¯ ~/.kaggle/kaggle.json ã‚’é…ç½®ã—ã¦ãã ã•ã„\")\n",
    "else:\n",
    "    print(\"âœ“ Kaggleèªè¨¼OKï¼ˆç’°å¢ƒå¤‰æ•° or kaggle.jsonï¼‰\")\n",
    "\n",
    "# =========================\n",
    "# Kaggleã‹ã‚‰DLï¼ˆå¿…è¦ãªã‚‰ï¼‰- CLIä½¿ç”¨ï¼ˆRAMæ­»ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰\n",
    "# =========================\n",
    "if RAW_ZIP_PATH is None or not Path(RAW_ZIP_PATH).exists():\n",
    "    print(f\"\\nğŸ“¥ Kaggleã‹ã‚‰DLä¸­: {KAGGLE_DATASET}\")\n",
    "    print(\"  CLIã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ï¼ˆPython APIã‚ˆã‚Šå®‰å®šï¼‰\")\n",
    "    KAGGLE_DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # kaggle CLIã‚³ãƒãƒ³ãƒ‰ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "    subprocess.run(\n",
    "        [\"kaggle\", \"datasets\", \"download\", \"-d\", KAGGLE_DATASET, \"-p\", str(KAGGLE_DOWNLOAD_DIR)],\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "    zip_files = sorted(KAGGLE_DOWNLOAD_DIR.glob(\"*.zip\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if zip_files:\n",
    "        RAW_ZIP_PATH = str(zip_files[0])\n",
    "        size_gb = zip_files[0].stat().st_size / (1024 ** 3)\n",
    "        print(f\"âœ“ DLå®Œäº†: {RAW_ZIP_PATH}\")\n",
    "        print(f\"  ã‚µã‚¤ã‚º: {size_gb:.2f} GB\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"zipãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆDLå…ˆã‚’ç¢ºèªã—ã¦ï¼‰\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ æ—¢å­˜zipã‚’ä½¿ç”¨: {RAW_ZIP_PATH}\")\n",
    "\n",
    "print(f\"\\næœ€çµ‚RAW_ZIP_PATH: {RAW_ZIP_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad31c40",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«4: unzipï¼ˆ/content/tmpã¸ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# unzipï¼ˆ/content/tmpã¸å±•é–‹ï¼‰\n",
    "# ============================================\n",
    "\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "if RAW_ZIP_PATH is None or not Path(RAW_ZIP_PATH).exists():\n",
    "    raise FileNotFoundError(f\"RAW_ZIP_PATHãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {RAW_ZIP_PATH}\")\n",
    "\n",
    "# å±•é–‹æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯\n",
    "if RAW_DIR.exists() and any(RAW_DIR.iterdir()):\n",
    "    print(f\"âœ“ æ—¢ã«å±•é–‹æ¸ˆã¿: {RAW_DIR}\")\n",
    "    print(f\"  ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")\n",
    "else:\n",
    "    print(f\"ğŸ“¦ zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹ä¸­...\")\n",
    "    print(f\"  ã‚½ãƒ¼ã‚¹: {RAW_ZIP_PATH}\")\n",
    "    print(f\"  å±•é–‹å…ˆ: {RAW_DIR}\")\n",
    "    \n",
    "    RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã\n",
    "    with zipfile.ZipFile(RAW_ZIP_PATH, 'r') as zip_ref:\n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "        file_list = zip_ref.namelist()\n",
    "        \n",
    "        # é€²æ—ãƒãƒ¼ä»˜ãã§å±•é–‹\n",
    "        for file_info in tqdm(file_list, desc=\"å±•é–‹ä¸­\", unit=\"ãƒ•ã‚¡ã‚¤ãƒ«\"):\n",
    "            zip_ref.extract(file_info, RAW_DIR)\n",
    "    \n",
    "    print(f\"\\nâœ“ å±•é–‹å®Œäº†: {RAW_DIR}\")\n",
    "    \n",
    "    # å±•é–‹çµæœã®ç¢ºèª\n",
    "    json_files = list(RAW_DIR.rglob(\"kaggle_data_*.json\"))\n",
    "    print(f\"  è¦‹ã¤ã‹ã£ãŸJSONãƒ•ã‚¡ã‚¤ãƒ«: {len(json_files)} ä»¶\")\n",
    "    \n",
    "    # background_imagesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ã™\n",
    "    bg_dirs = list(RAW_DIR.rglob(\"background_images\"))\n",
    "    print(f\"  è¦‹ã¤ã‹ã£ãŸbackground_imagesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {len(bg_dirs)} ä»¶\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5d1c3",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«5: rawã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcaad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# rawã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ\n",
    "# ============================================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’è¿½åŠ ï¼ˆconfigã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ï¼‰\n",
    "repo_path = Path(\"/content/last_assignment_progzissen\")\n",
    "if not repo_path.exists():\n",
    "    # Gitãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ï¼ˆã¾ã ã®å ´åˆï¼‰\n",
    "    import subprocess\n",
    "    print(\"ğŸ“¦ Gitãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­...\")\n",
    "    subprocess.run([\"git\", \"clone\", \"-b\", \"feature/onnx_recognizer\", \n",
    "                    \"https://github.com/masararutaru/last_assignment_progzissen.git\", \n",
    "                    str(repo_path)], check=True)\n",
    "    print(\"âœ“ ã‚¯ãƒ­ãƒ¼ãƒ³å®Œäº†\")\n",
    "\n",
    "sys.path.insert(0, str(repo_path / \"python\" / \"data\"))\n",
    "from config import CLASSES, CLASS_TO_ID, latex_to_class\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"rawãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢\n",
    "json_files = list(RAW_DIR.rglob(\"kaggle_data_*.json\"))\n",
    "print(f\"\\nè¦‹ã¤ã‹ã£ãŸJSONãƒ•ã‚¡ã‚¤ãƒ«: {len(json_files)} ä»¶\")\n",
    "\n",
    "if len(json_files) == 0:\n",
    "    raise FileNotFoundError(f\"JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {RAW_DIR}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’æ§‹ç¯‰\n",
    "# samples: {uuid: {'latex_chars': [...], 'bboxes': [...], 'image_path': Path}}\n",
    "samples = {}\n",
    "image_path_cache = {}  # {uuid: Path}\n",
    "\n",
    "# å„JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†\n",
    "print(\"\\nJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "for json_path in tqdm(json_files, desc=\"JSONå‡¦ç†\"):\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if not isinstance(data, list):\n",
    "            continue\n",
    "        \n",
    "        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰background_imagesã‚’æ¢ã™\n",
    "        json_dir = json_path.parent\n",
    "        parent_dir = json_dir.parent\n",
    "        background_images_dir = parent_dir / \"background_images\"\n",
    "        \n",
    "        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆã“ã®batchå†…ï¼‰\n",
    "        if background_images_dir.exists():\n",
    "            for img_file in background_images_dir.glob(\"*\"):\n",
    "                if img_file.is_file():\n",
    "                    uuid_key = img_file.stem\n",
    "                    if uuid_key not in image_path_cache:\n",
    "                        image_path_cache[uuid_key] = img_file\n",
    "        \n",
    "        # å„ã‚µãƒ³ãƒ—ãƒ«ã‚’å‡¦ç†\n",
    "        for sample in data:\n",
    "            if not isinstance(sample, dict):\n",
    "                continue\n",
    "            \n",
    "            uuid = sample.get('uuid', '')\n",
    "            if not uuid:\n",
    "                continue\n",
    "            \n",
    "            if 'image_data' not in sample:\n",
    "                continue\n",
    "            \n",
    "            img_data = sample['image_data']\n",
    "            visible_chars = img_data.get('visible_latex_chars', [])\n",
    "            xmins = img_data.get('xmins', [])\n",
    "            xmaxs = img_data.get('xmaxs', [])\n",
    "            ymins = img_data.get('ymins', [])\n",
    "            ymaxs = img_data.get('ymaxs', [])\n",
    "            \n",
    "            # bboxã®æ•°ã‚’ç¢ºèª\n",
    "            if len(visible_chars) != len(xmins) or len(visible_chars) != len(xmaxs) or \\\n",
    "               len(visible_chars) != len(ymins) or len(visible_chars) != len(ymaxs):\n",
    "                continue\n",
    "            \n",
    "            # ç”»åƒãƒ‘ã‚¹ã‚’æ¤œç´¢\n",
    "            image_path = image_path_cache.get(uuid)\n",
    "            if image_path is None:\n",
    "                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãªã„å ´åˆã¯ç›´æ¥æ¤œç´¢\n",
    "                if background_images_dir.exists():\n",
    "                    for ext in ['.png', '.jpg', '.jpeg']:\n",
    "                        test_path = background_images_dir / f\"{uuid}{ext}\"\n",
    "                        if test_path.exists():\n",
    "                            image_path = test_path\n",
    "                            image_path_cache[uuid] = image_path\n",
    "                            break\n",
    "            \n",
    "            # ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¿å­˜\n",
    "            samples[uuid] = {\n",
    "                'latex_chars': visible_chars,\n",
    "                'bboxes': list(zip(xmins, ymins, xmaxs, ymaxs)),  # (xmin, ymin, xmax, ymax)\n",
    "                'image_path': image_path\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš ï¸  è­¦å‘Š: {json_path} ã®å‡¦ç†ã«å¤±æ•—: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nâœ“ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†\")\n",
    "print(f\"  ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(samples)} ä»¶\")\n",
    "print(f\"  ç”»åƒãŒè¦‹ã¤ã‹ã£ãŸã‚µãƒ³ãƒ—ãƒ«: {sum(1 for s in samples.values() if s['image_path'] is not None)} ä»¶\")\n",
    "print(f\"  ç”»åƒãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‚µãƒ³ãƒ—ãƒ«: {sum(1 for s in samples.values() if s['image_path'] is None)} ä»¶\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc0271",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«6: ã‚¯ãƒ©ã‚¹æŠ½å‡ºãƒ»æ­£è¦åŒ–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e107fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ã‚¯ãƒ©ã‚¹æŠ½å‡ºãƒ»æ­£è¦åŒ–\n",
    "# ============================================\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ã‚¯ãƒ©ã‚¹æŠ½å‡ºãƒ»æ­£è¦åŒ–\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# å¯¾è±¡ã‚¯ãƒ©ã‚¹ã®IDãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆï¼ˆ0å§‹ã¾ã‚Šé€£ç•ªã«å†ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰\n",
    "target_class_to_new_id = {cls: idx for idx, cls in enumerate(TARGET_CLASSES)}\n",
    "print(f\"\\nå¯¾è±¡ã‚¯ãƒ©ã‚¹æ•°: {len(TARGET_CLASSES)}\")\n",
    "print(f\"  ã‚¯ãƒ©ã‚¹: {', '.join(TARGET_CLASSES)}\")\n",
    "\n",
    "# æŠ½å‡ºãƒ»å¤‰æ›å¾Œã®ãƒ‡ãƒ¼ã‚¿\n",
    "# processed_samples: {uuid: {'class_ids': [...], 'bboxes_yolo': [...], 'image_path': Path}}\n",
    "processed_samples = {}\n",
    "\n",
    "stats = defaultdict(int)  # ã‚¯ãƒ©ã‚¹åˆ¥çµ±è¨ˆ\n",
    "skipped_no_target = 0  # å¯¾è±¡ã‚¯ãƒ©ã‚¹ãŒãªã„ã‚µãƒ³ãƒ—ãƒ«\n",
    "skipped_invalid_bbox = 0  # ç„¡åŠ¹ãªbbox\n",
    "skipped_no_image = 0  # ç”»åƒãŒè¦‹ã¤ã‹ã‚‰ãªã„\n",
    "\n",
    "print(\"\\nã‚µãƒ³ãƒ—ãƒ«ã‚’å‡¦ç†ä¸­...\")\n",
    "for uuid, sample in tqdm(samples.items(), desc=\"ã‚¯ãƒ©ã‚¹æŠ½å‡º\"):\n",
    "    # ç”»åƒãƒ‘ã‚¹ã®ç¢ºèª\n",
    "    image_path = sample['image_path']\n",
    "    if image_path is None or not image_path.exists():\n",
    "        skipped_no_image += 1\n",
    "        continue\n",
    "    \n",
    "    latex_chars = sample['latex_chars']\n",
    "    bboxes = sample['bboxes']\n",
    "    \n",
    "    # å¯¾è±¡ã‚¯ãƒ©ã‚¹ã®ã¿æŠ½å‡º\n",
    "    class_ids = []\n",
    "    bboxes_yolo = []\n",
    "    \n",
    "    # é–¢æ•°åãƒªã‚¹ãƒˆ\n",
    "    function_names = ['sin', 'cos', 'tan', 'sec', 'csc', 'cot', 'ln', 'log', 'exp', 'sqrt', 'lim', 'abs']\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(latex_chars):\n",
    "        # ã¾ãšã€LaTeXã‚³ãƒãƒ³ãƒ‰å½¢å¼ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ'\\lim_', '\\sqrt'ãªã©ï¼‰\n",
    "        latex_char = latex_chars[i]\n",
    "        class_char_from_latex = latex_to_class(latex_char)\n",
    "        \n",
    "        # LaTeXã‚³ãƒãƒ³ãƒ‰ãŒé–¢æ•°åã¨ã—ã¦èªè­˜ã•ã‚ŒãŸå ´åˆ\n",
    "        if class_char_from_latex in function_names:\n",
    "            matched_function = class_char_from_latex\n",
    "            matched_length = 1  # LaTeXã‚³ãƒãƒ³ãƒ‰ã¯1ã¤ã®æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¦ã„ã‚‹\n",
    "        else:\n",
    "            # é–¢æ•°åã®ãƒãƒƒãƒãƒ³ã‚°ã‚’è©¦è¡Œï¼ˆé€£ç¶šã™ã‚‹æ–‡å­—åˆ—: 'l', 'i', 'm' â†’ 'lim'ï¼‰\n",
    "            matched_function = None\n",
    "            matched_length = 0\n",
    "            \n",
    "            for func_name in function_names:\n",
    "                if i + len(func_name) <= len(latex_chars):\n",
    "                    # é€£ç¶šã™ã‚‹æ–‡å­—åˆ—ãŒé–¢æ•°åã¨ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "                    candidate = ''.join(latex_chars[i:i+len(func_name)])\n",
    "                    if candidate == func_name:\n",
    "                        matched_function = func_name\n",
    "                        matched_length = len(func_name)\n",
    "                        break\n",
    "        \n",
    "        if matched_function:\n",
    "            # é–¢æ•°åã¨ã—ã¦å‡¦ç†\n",
    "            start_idx = i\n",
    "            end_idx = i + matched_length - 1\n",
    "            \n",
    "            if end_idx < len(bboxes):\n",
    "                # é–¢æ•°å…¨ä½“ã®bboxã‚’è¨ˆç®—\n",
    "                xmin = min(bboxes[j][0] for j in range(start_idx, end_idx+1))\n",
    "                ymin = min(bboxes[j][1] for j in range(start_idx, end_idx+1))\n",
    "                xmax = max(bboxes[j][2] for j in range(start_idx, end_idx+1))\n",
    "                ymax = max(bboxes[j][3] for j in range(start_idx, end_idx+1))\n",
    "                \n",
    "                # æ–°ã—ã„ã‚¯ãƒ©ã‚¹IDã‚’å–å¾—\n",
    "                new_class_id = target_class_to_new_id[matched_function]\n",
    "                class_char = matched_function\n",
    "            else:\n",
    "                i += matched_length\n",
    "                continue\n",
    "        else:\n",
    "            # é€šå¸¸ã®æ–‡å­—ã¨ã—ã¦å‡¦ç†\n",
    "            latex_char = latex_chars[i]\n",
    "            class_char = latex_to_class(latex_char)\n",
    "            \n",
    "            if class_char is None or class_char not in TARGET_CLASSES:\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # æ–°ã—ã„ã‚¯ãƒ©ã‚¹IDã‚’å–å¾—ï¼ˆ0å§‹ã¾ã‚Šé€£ç•ªï¼‰\n",
    "            new_class_id = target_class_to_new_id[class_char]\n",
    "            \n",
    "            # bboxã‚’å–å¾—\n",
    "            xmin, ymin, xmax, ymax = bboxes[i]\n",
    "        \n",
    "        # ç„¡åŠ¹ãªbboxã‚’ãƒã‚§ãƒƒã‚¯\n",
    "        if xmin >= xmax or ymin >= ymax:\n",
    "            skipped_invalid_bbox += 1\n",
    "            if matched_function:\n",
    "                i += matched_length\n",
    "            else:\n",
    "                i += 1\n",
    "            continue\n",
    "        \n",
    "        # 0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—\n",
    "        xmin = max(0.0, min(1.0, xmin))\n",
    "        ymin = max(0.0, min(1.0, ymin))\n",
    "        xmax = max(0.0, min(1.0, xmax))\n",
    "        ymax = max(0.0, min(1.0, ymax))\n",
    "        \n",
    "        # YOLOå½¢å¼ã«å¤‰æ›ï¼ˆcenter_x, center_y, width, heightï¼‰\n",
    "        center_x = (xmin + xmax) / 2.0\n",
    "        center_y = (ymin + ymax) / 2.0\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        \n",
    "        # 0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—\n",
    "        center_x = max(0.0, min(1.0, center_x))\n",
    "        center_y = max(0.0, min(1.0, center_y))\n",
    "        width = max(0.0, min(1.0, width))\n",
    "        height = max(0.0, min(1.0, height))\n",
    "        \n",
    "        # ã¯ã¿å‡ºã—ãƒã‚§ãƒƒã‚¯ï¼ˆx_centerÂ±w/2, y_centerÂ±h/2 ãŒ 0-1 å†…ï¼‰\n",
    "        if center_x - width/2 < 0 or center_x + width/2 > 1 or \\\n",
    "           center_y - height/2 < 0 or center_y + height/2 > 1:\n",
    "            # ã‚¯ãƒªãƒƒãƒ—ã—ã¦å†è¨ˆç®—\n",
    "            xmin_clipped = max(0.0, center_x - width/2)\n",
    "            xmax_clipped = min(1.0, center_x + width/2)\n",
    "            ymin_clipped = max(0.0, center_y - height/2)\n",
    "            ymax_clipped = min(1.0, center_y + height/2)\n",
    "            \n",
    "            center_x = (xmin_clipped + xmax_clipped) / 2.0\n",
    "            center_y = (ymin_clipped + ymax_clipped) / 2.0\n",
    "            width = xmax_clipped - xmin_clipped\n",
    "            height = ymax_clipped - ymin_clipped\n",
    "        \n",
    "        # ã‚¼ãƒ­é¢ç©ãƒã‚§ãƒƒã‚¯\n",
    "        if width <= 0 or height <= 0:\n",
    "            skipped_invalid_bbox += 1\n",
    "            if matched_function:\n",
    "                i += matched_length\n",
    "            else:\n",
    "                i += 1\n",
    "            continue\n",
    "        \n",
    "        class_ids.append(new_class_id)\n",
    "        bboxes_yolo.append((center_x, center_y, width, height))\n",
    "        stats[class_char] += 1\n",
    "        \n",
    "        if matched_function:\n",
    "            i += matched_length\n",
    "        else:\n",
    "            i += 1\n",
    "    # å¯¾è±¡ã‚¯ãƒ©ã‚¹ãŒ1ã¤ã‚‚ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "    if len(class_ids) == 0:\n",
    "        skipped_no_target += 1\n",
    "        continue\n",
    "    \n",
    "    # å‡¦ç†æ¸ˆã¿ã‚µãƒ³ãƒ—ãƒ«ã«è¿½åŠ \n",
    "    processed_samples[uuid] = {\n",
    "        'class_ids': class_ids,\n",
    "        'bboxes_yolo': bboxes_yolo,\n",
    "        'image_path': image_path\n",
    "    }\n",
    "\n",
    "print(f\"\\nâœ“ ã‚¯ãƒ©ã‚¹æŠ½å‡ºãƒ»æ­£è¦åŒ–å®Œäº†\")\n",
    "print(f\"  å‡¦ç†æ¸ˆã¿ã‚µãƒ³ãƒ—ãƒ«: {len(processed_samples)} ä»¶\")\n",
    "print(f\"  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå¯¾è±¡ã‚¯ãƒ©ã‚¹ãªã—ï¼‰: {skipped_no_target} ä»¶\")\n",
    "print(f\"  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç„¡åŠ¹bboxï¼‰: {skipped_invalid_bbox} ä»¶\")\n",
    "print(f\"  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç”»åƒãªã—ï¼‰: {skipped_no_image} ä»¶\")\n",
    "\n",
    "print(f\"\\nã‚¯ãƒ©ã‚¹åˆ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ•°:\")\n",
    "for cls in TARGET_CLASSES:\n",
    "    count = stats[cls]\n",
    "    print(f\"  {cls:>3}: {count:>8} ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\")\n",
    "\n",
    "total_instances = sum(stats.values())\n",
    "print(f\"\\nç·ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ•°: {total_instances}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab0706",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«7: train/val split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deea15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# train/val split\n",
    "# ============================================\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"train/val split\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
    "random.seed(SEED)\n",
    "\n",
    "# UUIDã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã¦ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "uuids = list(processed_samples.keys())\n",
    "random.shuffle(uuids)\n",
    "\n",
    "# åˆ†å‰²\n",
    "split_idx = int(len(uuids) * (1 - VAL_RATIO))\n",
    "train_uuids = uuids[:split_idx]\n",
    "val_uuids = uuids[split_idx:]\n",
    "\n",
    "print(f\"\\nåˆ†å‰²çµæœ:\")\n",
    "print(f\"  Train: {len(train_uuids)} ä»¶ ({len(train_uuids)/len(uuids)*100:.1f}%)\")\n",
    "print(f\"  Val: {len(val_uuids)} ä»¶ ({len(val_uuids)/len(uuids)*100:.1f}%)\")\n",
    "print(f\"  ç·æ•°: {len(uuids)} ä»¶\")\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã®ç¢ºèªï¼ˆç°¡æ˜“ã‚µãƒãƒªï¼‰\n",
    "train_class_counts = Counter()\n",
    "val_class_counts = Counter()\n",
    "\n",
    "for uuid in train_uuids:\n",
    "    for class_id in processed_samples[uuid]['class_ids']:\n",
    "        train_class_counts[class_id] += 1\n",
    "\n",
    "for uuid in val_uuids:\n",
    "    for class_id in processed_samples[uuid]['class_ids']:\n",
    "        val_class_counts[class_id] += 1\n",
    "\n",
    "print(f\"\\nã‚¯ãƒ©ã‚¹åˆ†å¸ƒï¼ˆç°¡æ˜“ã‚µãƒãƒªï¼‰:\")\n",
    "print(f\"  Train ç·ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: {sum(train_class_counts.values())}\")\n",
    "print(f\"  Val ç·ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: {sum(val_class_counts.values())}\")\n",
    "\n",
    "print(f\"\\nâœ“ train/val splitå®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9dd8ff",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«8: YOLOãƒ•ã‚©ãƒ«ãƒ€å‡ºåŠ›\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81edc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOLOãƒ•ã‚©ãƒ«ãƒ€å‡ºåŠ›\n",
    "# ============================================\n",
    "\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"YOLOãƒ•ã‚©ãƒ«ãƒ€å‡ºåŠ›\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™\n",
    "if CLEAN_OUT_DIR and OUT_DIR.exists():\n",
    "    print(f\"ğŸ—‘ï¸  æ—¢å­˜ã®OUT_DIRã‚’å‰Šé™¤: {OUT_DIR}\")\n",
    "    shutil.rmtree(OUT_DIR)\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ\n",
    "train_images_dir = OUT_DIR / \"train\" / \"images\"\n",
    "train_labels_dir = OUT_DIR / \"train\" / \"labels\"\n",
    "val_images_dir = OUT_DIR / \"val\" / \"images\"\n",
    "val_labels_dir = OUT_DIR / \"val\" / \"labels\"\n",
    "\n",
    "for d in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nå‡ºåŠ›å…ˆ: {OUT_DIR}\")\n",
    "\n",
    "# trainãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›\n",
    "print(f\"\\nğŸ“ trainãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ä¸­...\")\n",
    "train_mapping = {}\n",
    "\n",
    "for uuid in tqdm(train_uuids, desc=\"train\"):\n",
    "    sample = processed_samples[uuid]\n",
    "    image_path = sample['image_path']\n",
    "    \n",
    "    # ç”»åƒã‚’ã‚³ãƒ”ãƒ¼ï¼ˆæ‹¡å¼µå­ã‚’ä¿æŒï¼‰\n",
    "    ext = image_path.suffix.lower()\n",
    "    if ext not in ['.png', '.jpg', '.jpeg']:\n",
    "        ext = '.png'\n",
    "    \n",
    "    target_image_path = train_images_dir / f\"{uuid}{ext}\"\n",
    "    if not target_image_path.exists():\n",
    "        shutil.copy2(image_path, target_image_path)\n",
    "    \n",
    "    # ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›\n",
    "    label_path = train_labels_dir / f\"{uuid}.txt\"\n",
    "    with open(label_path, 'w', encoding='utf-8') as f:\n",
    "        for class_id, (cx, cy, w, h) in zip(sample['class_ids'], sample['bboxes_yolo']):\n",
    "            f.write(f\"{class_id} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "    \n",
    "    train_mapping[uuid] = str(target_image_path)\n",
    "\n",
    "# valãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›\n",
    "print(f\"\\nğŸ“ valãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ä¸­...\")\n",
    "val_mapping = {}\n",
    "\n",
    "for uuid in tqdm(val_uuids, desc=\"val\"):\n",
    "    sample = processed_samples[uuid]\n",
    "    image_path = sample['image_path']\n",
    "    \n",
    "    # ç”»åƒã‚’ã‚³ãƒ”ãƒ¼\n",
    "    ext = image_path.suffix.lower()\n",
    "    if ext not in ['.png', '.jpg', '.jpeg']:\n",
    "        ext = '.png'\n",
    "    \n",
    "    target_image_path = val_images_dir / f\"{uuid}{ext}\"\n",
    "    if not target_image_path.exists():\n",
    "        shutil.copy2(image_path, target_image_path)\n",
    "    \n",
    "    # ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›\n",
    "    label_path = val_labels_dir / f\"{uuid}.txt\"\n",
    "    with open(label_path, 'w', encoding='utf-8') as f:\n",
    "        for class_id, (cx, cy, w, h) in zip(sample['class_ids'], sample['bboxes_yolo']):\n",
    "            f.write(f\"{class_id} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "    \n",
    "    val_mapping[uuid] = str(target_image_path)\n",
    "\n",
    "# mapping.jsonã‚’ç”Ÿæˆ\n",
    "mapping = {\n",
    "    'train': train_mapping,\n",
    "    'val': val_mapping\n",
    "}\n",
    "\n",
    "mapping_file = OUT_DIR / \"mapping.json\"\n",
    "with open(mapping_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ mapping.jsonã‚’ç”Ÿæˆ: {mapping_file}\")\n",
    "\n",
    "# data.yamlã‚’ç”Ÿæˆï¼ˆYOLOãŒèª­ã‚€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼‰\n",
    "data_yaml = {\n",
    "    'path': str(OUT_DIR),\n",
    "    'train': 'train/images',\n",
    "    'val': 'val/images',\n",
    "    'nc': len(TARGET_CLASSES),\n",
    "    'names': {idx: cls for idx, cls in enumerate(TARGET_CLASSES)}\n",
    "}\n",
    "\n",
    "data_yaml_file = OUT_DIR / \"data.yaml\"\n",
    "with open(data_yaml_file, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(data_yaml, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"\\nâœ“ data.yamlã‚’ç”Ÿæˆ: {data_yaml_file}\")\n",
    "\n",
    "# ä»¶æ•°ãƒã‚§ãƒƒã‚¯\n",
    "train_images = list(train_images_dir.glob(\"*\"))\n",
    "train_labels = list(train_labels_dir.glob(\"*.txt\"))\n",
    "val_images = list(val_images_dir.glob(\"*\"))\n",
    "val_labels = list(val_labels_dir.glob(\"*.txt\"))\n",
    "\n",
    "print(f\"\\nå‡ºåŠ›çµæœ:\")\n",
    "print(f\"  train/images: {len(train_images)} ä»¶\")\n",
    "print(f\"  train/labels: {len(train_labels)} ä»¶\")\n",
    "print(f\"  val/images: {len(val_images)} ä»¶\")\n",
    "print(f\"  val/labels: {len(val_labels)} ä»¶\")\n",
    "\n",
    "# 1å¯¾1å¯¾å¿œã®ç¢ºèª\n",
    "train_image_uuids = {f.stem for f in train_images}\n",
    "train_label_uuids = {f.stem for f in train_labels}\n",
    "val_image_uuids = {f.stem for f in val_images}\n",
    "val_label_uuids = {f.stem for f in val_labels}\n",
    "\n",
    "train_matched = train_image_uuids & train_label_uuids\n",
    "val_matched = val_image_uuids & val_label_uuids\n",
    "\n",
    "print(f\"\\n1å¯¾1å¯¾å¿œç¢ºèª:\")\n",
    "print(f\"  train: {len(train_matched)}/{len(train_images)} ãƒšã‚¢\")\n",
    "print(f\"  val: {len(val_matched)}/{len(val_images)} ãƒšã‚¢\")\n",
    "\n",
    "if len(train_matched) == len(train_images) == len(train_labels) and \\\n",
    "   len(val_matched) == len(val_images) == len(val_labels):\n",
    "    print(f\"\\nâœ“ å®Œç’§ï¼å…¨ã¦ã®ç”»åƒã¨ãƒ©ãƒ™ãƒ«ãŒ1å¯¾1ã§å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  è­¦å‘Š: ä¸€éƒ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¯¾å¿œã—ã¦ã„ã¾ã›ã‚“\")\n",
    "\n",
    "print(f\"\\nâœ“ YOLOãƒ•ã‚©ãƒ«ãƒ€å‡ºåŠ›å®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d2d0d6",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«9: æ¤œè¨¼ï¼ˆé‡è¦ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e80af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# æ¤œè¨¼ï¼ˆé‡è¦ï¼‰\n",
    "# ============================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"æ¤œè¨¼\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ä»¶æ•°ãƒã‚§ãƒƒã‚¯\n",
    "print(f\"\\nä»¶æ•°ãƒã‚§ãƒƒã‚¯:\")\n",
    "print(f\"  train/images: {len(train_images)} ä»¶\")\n",
    "print(f\"  train/labels: {len(train_labels)} ä»¶\")\n",
    "print(f\"  val/images: {len(val_images)} ä»¶\")\n",
    "print(f\"  val/labels: {len(val_labels)} ä»¶\")\n",
    "\n",
    "# ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸ã‚“ã§è¡¨ç¤º\n",
    "num_samples = 5\n",
    "sample_uuids = random.sample(list(train_matched), min(num_samples, len(train_matched)))\n",
    "\n",
    "print(f\"\\nã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼ˆ{num_samples}ä»¶ï¼‰:\")\n",
    "\n",
    "fig, axes = plt.subplots(num_samples, 1, figsize=(12, 4 * num_samples))\n",
    "if num_samples == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, uuid in enumerate(sample_uuids):\n",
    "    # ç”»åƒã‚’èª­ã¿è¾¼ã¿\n",
    "    image_files = list(train_images_dir.glob(f\"{uuid}.*\"))\n",
    "    if len(image_files) == 0:\n",
    "        continue\n",
    "    \n",
    "    image_path = image_files[0]\n",
    "    img = Image.open(image_path)\n",
    "    img_width, img_height = img.size\n",
    "    \n",
    "    # ãƒ©ãƒ™ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "    label_path = train_labels_dir / f\"{uuid}.txt\"\n",
    "    labels = []\n",
    "    if label_path.exists():\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    center_x = float(parts[1])\n",
    "                    center_y = float(parts[2])\n",
    "                    width = float(parts[3])\n",
    "                    height = float(parts[4])\n",
    "                    \n",
    "                    # YOLOå½¢å¼ã‹ã‚‰ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™ã«å¤‰æ›\n",
    "                    x_center = center_x * img_width\n",
    "                    y_center = center_y * img_height\n",
    "                    bbox_width = width * img_width\n",
    "                    bbox_height = height * img_height\n",
    "                    \n",
    "                    xmin = x_center - bbox_width / 2\n",
    "                    ymin = y_center - bbox_height / 2\n",
    "                    xmax = x_center + bbox_width / 2\n",
    "                    ymax = y_center + bbox_height / 2\n",
    "                    \n",
    "                    class_name = TARGET_CLASSES[class_id] if class_id < len(TARGET_CLASSES) else 'unknown'\n",
    "                    labels.append({\n",
    "                        'class_id': class_id,\n",
    "                        'class_name': class_name,\n",
    "                        'bbox': (xmin, ymin, xmax, ymax)\n",
    "                    })\n",
    "    \n",
    "    # ç”»åƒã‚’è¡¨ç¤º\n",
    "    ax = axes[idx]\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’æç”»\n",
    "    colors = plt.cm.tab20(range(len(TARGET_CLASSES)))\n",
    "    for label in labels:\n",
    "        class_id = label['class_id']\n",
    "        class_name = label['class_name']\n",
    "        xmin, ymin, xmax, ymax = label['bbox']\n",
    "        \n",
    "        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "            linewidth=2, edgecolor=colors[class_id % len(colors)],\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # ã‚¯ãƒ©ã‚¹åã‚’è¡¨ç¤º\n",
    "        ax.text(\n",
    "            xmin, ymin - 5, class_name,\n",
    "            color=colors[class_id % len(colors)],\n",
    "            fontsize=10, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7)\n",
    "        )\n",
    "    \n",
    "    # ã‚¿ã‚¤ãƒˆãƒ«\n",
    "    title = f\"UUID: {uuid[:8]}... | æ¤œå‡ºæ•°: {len(labels)}\"\n",
    "    if len(labels) > 0:\n",
    "        classes_found = [l['class_name'] for l in labels]\n",
    "        title += f\" | ã‚¯ãƒ©ã‚¹: {', '.join(set(classes_found))}\"\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ãƒ©ãƒ™ãƒ«txtã®ä¸­èº«ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€åˆã®1ä»¶ï¼‰\n",
    "if len(sample_uuids) > 0:\n",
    "    uuid = sample_uuids[0]\n",
    "    label_path = train_labels_dir / f\"{uuid}.txt\"\n",
    "    print(f\"\\nãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«ç¢ºèªï¼ˆ{uuid}.txtï¼‰:\")\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        print(f\"  è¡Œæ•°: {len(lines)}\")\n",
    "        if len(lines) > 0:\n",
    "            print(f\"  æœ€åˆã®3è¡Œ:\")\n",
    "            for i, line in enumerate(lines[:3]):\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    class_name = TARGET_CLASSES[class_id] if class_id < len(TARGET_CLASSES) else 'unknown'\n",
    "                    print(f\"    {i+1}: class={class_name}({class_id}) cx={parts[1]} cy={parts[2]} w={parts[3]} h={parts[4]}\")\n",
    "\n",
    "print(f\"\\nâœ“ æ¤œè¨¼å®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22c648",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«10: manifest.jsonç”Ÿæˆï¼ˆå†ç¾æ€§ã®æ ¸ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc05e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# manifest.jsonç”Ÿæˆï¼ˆå†ç¾æ€§ã®æ ¸ï¼‰\n",
    "# ============================================\n",
    "\n",
    "import hashlib\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"manifest.jsonç”Ÿæˆ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Git commit hashã‚’å–å¾—ï¼ˆå¯èƒ½ãªã‚‰ï¼‰\n",
    "git_commit_hash = None\n",
    "try:\n",
    "    if repo_path.exists():\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"-C\", str(repo_path), \"rev-parse\", \"HEAD\"],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        git_commit_hash = result.stdout.strip()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# manifest.jsonã‚’ç”Ÿæˆï¼ˆtarãƒ•ã‚¡ã‚¤ãƒ«ã¯ã¾ã ä½œæˆã•ã‚Œã¦ã„ãªã„ãŸã‚ã€tar_filesã¯ç©ºï¼‰\n",
    "manifest = {\n",
    "    'version': '1.0',\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'kaggle_dataset': KAGGLE_DATASET if RAW_ZIP_PATH is None else None,\n",
    "    'raw_zip_path': str(RAW_ZIP_PATH) if RAW_ZIP_PATH else None,\n",
    "    'target_classes': TARGET_CLASSES,\n",
    "    'num_classes': len(TARGET_CLASSES),\n",
    "    'split': {\n",
    "        'seed': SEED,\n",
    "        'val_ratio': VAL_RATIO,\n",
    "        'train_count': len(train_uuids),\n",
    "        'val_count': len(val_uuids)\n",
    "    },\n",
    "    'statistics': {\n",
    "        'train_images': len(train_images),\n",
    "        'train_labels': len(train_labels),\n",
    "        'val_images': len(val_images),\n",
    "        'val_labels': len(val_labels)\n",
    "    },\n",
    "    'git_commit_hash': git_commit_hash,\n",
    "    'tar_files': {},  # tarãƒ•ã‚¡ã‚¤ãƒ«ã¯å¾Œã§ä½œæˆã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯ç©º\n",
    "    's3': {\n",
    "        'bucket': S3_BUCKET if UPLOAD_TO_S3 else None,\n",
    "        'prefix': S3_PREFIX if UPLOAD_TO_S3 else None,\n",
    "        'uploaded': False  # ã¾ã ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„\n",
    "    }\n",
    "}\n",
    "\n",
    "manifest_file = OUT_DIR / \"manifest.json\"\n",
    "with open(manifest_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ manifest.jsonã‚’ç”Ÿæˆ: {manifest_file}\")\n",
    "print(f\"\\nmanifest.jsonã®å†…å®¹:\")\n",
    "print(json.dumps(manifest, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(f\"\\nâœ“ manifest.jsonç”Ÿæˆå®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7929b",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«11: taråŒ–ï¼ˆS3ä¿ç®¡ç”¨ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6071c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# taråŒ–ï¼ˆS3ä¿ç®¡ç”¨ï¼‰\n",
    "# ============================================\n",
    "\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"taråŒ–ï¼ˆS3ä¿ç®¡ç”¨ï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# tarãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›å…ˆ\n",
    "tar_dir = OUT_DIR.parent / \"yolo_tars\"\n",
    "tar_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nå‡ºåŠ›å…ˆ: {tar_dir}\")\n",
    "\n",
    "# tarãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "train_images_tar = tar_dir / \"train_images.tar\"\n",
    "train_labels_tar = tar_dir / \"train_labels.tar\"\n",
    "val_images_tar = tar_dir / \"val_images.tar\"\n",
    "val_labels_tar = tar_dir / \"val_labels.tar\"\n",
    "meta_tar = tar_dir / \"meta.tar\"\n",
    "\n",
    "def create_tar(source_dir, tar_path, desc):\n",
    "    \"\"\"tarãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nğŸ“¦ {desc}ã‚’ä½œæˆä¸­...\")\n",
    "    \n",
    "    with tarfile.open(tar_path, 'w') as tar:\n",
    "        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ \n",
    "        for file_path in tqdm(list(source_dir.rglob('*')), desc=desc):\n",
    "            if file_path.is_file():\n",
    "                # ç›¸å¯¾ãƒ‘ã‚¹ã§è¿½åŠ \n",
    "                arcname = file_path.relative_to(source_dir)\n",
    "                tar.add(file_path, arcname=arcname)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    size_mb = tar_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  âœ“ å®Œäº†: {tar_path.name} ({size_mb:.2f} MB, {elapsed:.1f}ç§’)\")\n",
    "    return size_mb\n",
    "\n",
    "# train_images.tar\n",
    "if not train_images_tar.exists():\n",
    "    create_tar(train_images_dir, train_images_tar, \"train_images.tar\")\n",
    "else:\n",
    "    size_mb = train_images_tar.stat().st_size / (1024 * 1024)\n",
    "    print(f\"\\nâœ“ train_images.tar ã¯æ—¢ã«å­˜åœ¨: {size_mb:.2f} MB\")\n",
    "\n",
    "# train_labels.tar\n",
    "if not train_labels_tar.exists():\n",
    "    create_tar(train_labels_dir, train_labels_tar, \"train_labels.tar\")\n",
    "else:\n",
    "    size_mb = train_labels_tar.stat().st_size / (1024 * 1024)\n",
    "    print(f\"\\nâœ“ train_labels.tar ã¯æ—¢ã«å­˜åœ¨: {size_mb:.2f} MB\")\n",
    "\n",
    "# val_images.tar\n",
    "if not val_images_tar.exists():\n",
    "    create_tar(val_images_dir, val_images_tar, \"val_images.tar\")\n",
    "else:\n",
    "    size_mb = val_images_tar.stat().st_size / (1024 * 1024)\n",
    "    print(f\"\\nâœ“ val_images.tar ã¯æ—¢ã«å­˜åœ¨: {size_mb:.2f} MB\")\n",
    "\n",
    "# val_labels.tar\n",
    "if not val_labels_tar.exists():\n",
    "    create_tar(val_labels_dir, val_labels_tar, \"val_labels.tar\")\n",
    "else:\n",
    "    size_mb = val_labels_tar.stat().st_size / (1024 * 1024)\n",
    "    print(f\"\\nâœ“ val_labels.tar ã¯æ—¢ã«å­˜åœ¨: {size_mb:.2f} MB\")\n",
    "\n",
    "# meta.tarï¼ˆmapping.json, data.yaml, manifest.jsonï¼‰\n",
    "manifest_file = OUT_DIR / \"manifest.json\"\n",
    "if not manifest_file.exists():\n",
    "    raise FileNotFoundError(f\"manifest.jsonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {manifest_file}\\nã‚»ãƒ«10ï¼ˆmanifest.jsonç”Ÿæˆï¼‰ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "meta_files = [mapping_file, data_yaml_file, manifest_file]\n",
    "\n",
    "if not meta_tar.exists():\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nğŸ“¦ meta.tarã‚’ä½œæˆä¸­...\")\n",
    "    \n",
    "    with tarfile.open(meta_tar, 'w') as tar:\n",
    "        for file_path in meta_files:\n",
    "            if file_path.exists():\n",
    "                arcname = file_path.name\n",
    "                tar.add(file_path, arcname=arcname)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    size_mb = meta_tar.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  âœ“ å®Œäº†: {meta_tar.name} ({size_mb:.2f} MB, {elapsed:.1f}ç§’)\")\n",
    "else:\n",
    "    size_mb = meta_tar.stat().st_size / (1024 * 1024)\n",
    "    print(f\"\\nâœ“ meta.tar ã¯æ—¢ã«å­˜åœ¨: {size_mb:.2f} MB\")\n",
    "\n",
    "# åˆè¨ˆã‚µã‚¤ã‚º\n",
    "total_size = sum(f.stat().st_size for f in [train_images_tar, train_labels_tar, val_images_tar, val_labels_tar, meta_tar] if f.exists())\n",
    "total_size_mb = total_size / (1024 * 1024)\n",
    "print(f\"\\nâœ“ taråŒ–å®Œäº†\")\n",
    "print(f\"  åˆè¨ˆã‚µã‚¤ã‚º: {total_size_mb:.2f} MB\")\n",
    "print(f\"  å‡ºåŠ›å…ˆ: {tar_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7b59a",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«12: S3ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆä»»æ„ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb04d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# S3ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆä»»æ„ï¼‰\n",
    "# ============================================\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "if not UPLOAD_TO_S3:\n",
    "    print(\"âš ï¸  UPLOAD_TO_S3=False ã®ãŸã‚ã€S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")\n",
    "    print(f\"   ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆã¯ã€ã‚»ãƒ«1ã®è¨­å®šã§ UPLOAD_TO_S3=True ã«ã—ã¦ãã ã•ã„\")\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"S3ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # AWSèªè¨¼æƒ…å ±ã®ç¢ºèª\n",
    "    try:\n",
    "        result = subprocess.run([\"aws\", \"sts\", \"get-caller-identity\"], \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        print(f\"\\nâœ“ AWSèªè¨¼æƒ…å ±ã‚’ç¢ºèª\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"\\nâŒ ã‚¨ãƒ©ãƒ¼: AWSèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "        print(\"   aws configure ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„\")\n",
    "        raise\n",
    "    \n",
    "    tar_files = [\n",
    "        (train_images_tar, \"train_images.tar\"),\n",
    "        (train_labels_tar, \"train_labels.tar\"),\n",
    "        (val_images_tar, \"val_images.tar\"),\n",
    "        (val_labels_tar, \"val_labels.tar\"),\n",
    "        (meta_tar, \"meta.tar\")\n",
    "    ]\n",
    "    \n",
    "    uploaded = []\n",
    "    failed = []\n",
    "    \n",
    "    for tar_path, tar_name in tar_files:\n",
    "        if not tar_path.exists():\n",
    "            print(f\"\\nâš ï¸  ã‚¹ã‚­ãƒƒãƒ—: {tar_name} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "            continue\n",
    "        \n",
    "        s3_path = f\"s3://{S3_BUCKET}/{S3_PREFIX}/{tar_name}\"\n",
    "        \n",
    "        print(f\"\\nğŸ“¤ {tar_name} ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "        print(f\"   S3ãƒ‘ã‚¹: {s3_path}\")\n",
    "        \n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        success = False\n",
    "        \n",
    "        while retry_count < max_retries and not success:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                result = subprocess.run(\n",
    "                    [\"aws\", \"s3\", \"cp\", str(tar_path), s3_path],\n",
    "                    capture_output=True, text=True, check=True\n",
    "                )\n",
    "                elapsed = time.time() - start_time\n",
    "                size_mb = tar_path.stat().st_size / (1024 * 1024)\n",
    "                print(f\"  âœ“ å®Œäº†: {size_mb:.2f} MB, {elapsed:.1f}ç§’\")\n",
    "                uploaded.append(tar_name)\n",
    "                success = True\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                retry_count += 1\n",
    "                if retry_count < max_retries:\n",
    "                    print(f\"  âš ï¸  ãƒªãƒˆãƒ©ã‚¤ {retry_count}/{max_retries}...\")\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    print(f\"  âŒ å¤±æ•—: {e.stderr}\")\n",
    "                    failed.append(tar_name)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"  æˆåŠŸ: {len(uploaded)} ä»¶\")\n",
    "    if uploaded:\n",
    "        for name in uploaded:\n",
    "            print(f\"    âœ“ {name}\")\n",
    "    \n",
    "    print(f\"  å¤±æ•—: {len(failed)} ä»¶\")\n",
    "    if failed:\n",
    "        for name in failed:\n",
    "            print(f\"    âŒ {name}\")\n",
    "    \n",
    "    if len(uploaded) == len(tar_files):\n",
    "        print(f\"\\nâœ“ å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "        print(f\"  S3ãƒ‘ã‚¹: s3://{S3_BUCKET}/{S3_PREFIX}/\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  ä¸€éƒ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "        print(f\"  å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯æ‰‹å‹•ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
