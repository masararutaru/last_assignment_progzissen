{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# æ‹¬å¼§ç”¨2å›ç›®Finetuneï¼ˆfliplr=0.0 + Aidaãƒ‡ãƒ¼ã‚¿çµ±åˆï¼‰\n",
        "\n",
        "## ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ç›®çš„\n",
        "\n",
        "`prepare_paren_data.ipynb`ã§ä½œæˆã—ãŸæ‹¬å¼§ã®æ“¬ä¼¼æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã¨**Aidaãƒ‡ãƒ¼ã‚¿ã‚’1:4ã®æ¯”ç‡ã§æ··ãœã¦**ã€\n",
        "**2å›ç›®ã®finetune**ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚`fliplr=0.0`ï¼ˆå·¦å³åè»¢ãªã—ï¼‰ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€\n",
        "`)` ãŒ `(` ã«å¸ã‚ã‚Œã‚‹å•é¡Œã‚’ä¿®æ­£ã—ã¾ã™ã€‚\n",
        "\n",
        "### å‡¦ç†ã®æµã‚Œ\n",
        "1. AWSèªè¨¼è¨­å®š\n",
        "2. YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
        "3. S3ã‹ã‚‰æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ã¨Aidaãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å±•é–‹\n",
        "4. Aidaãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ‹¬å¼§ã‚¯ãƒ©ã‚¹ï¼ˆ42, 43ï¼‰ã‚’é™¤å¤–\n",
        "5. å„ã‚¯ãƒ©ã‚¹ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ–°:æ—§ = 1:4ï¼‰\n",
        "6. ãƒ‡ãƒ¼ã‚¿ã‚’train/valã«åˆ†å‰²ï¼ˆval_oldã¨val_newã‚’åˆ¥ã€…ã«è©•ä¾¡å¯èƒ½ï¼‰\n",
        "7. data.yamlã®ä½œæˆ\n",
        "8. hyp.yamlã®ä½œæˆï¼ˆfliplr=0.0è¨­å®šï¼‰\n",
        "9. æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‹ã‚‰finetuneã‚’å®Ÿè¡Œ\n",
        "10. å­¦ç¿’çµæœã®ç¢ºèª\n",
        "11. ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "\n",
        "### æ³¨æ„ç‚¹\n",
        "- **ãƒ‡ãƒ¼ã‚¿æ¯”ç‡**: æ–°ãƒ‡ãƒ¼ã‚¿ï¼ˆæ‹¬å¼§ï¼‰: æ—§ãƒ‡ãƒ¼ã‚¿ï¼ˆAidaï¼‰ = 1:4\n",
        "- **Aidaãƒ‡ãƒ¼ã‚¿ã®æ‹¬å¼§ã¯é™¤å¤–**: Aidaãƒ‡ãƒ¼ã‚¿ã®æ‹¬å¼§ã‚¯ãƒ©ã‚¹ï¼ˆ42, 43ï¼‰ã¯ä½¿ç”¨ã—ãªã„ï¼ˆfliplr=0.0ã®ãŸã‚ï¼‰\n",
        "- **fliplr=0.0**: å·¦å³åè»¢ã‚’ç„¡åŠ¹åŒ–ï¼ˆæ‹¬å¼§ã®è­˜åˆ¥å¢ƒç•Œã‚’çŸ¯æ­£ã™ã‚‹ãŸã‚ï¼‰\n",
        "- **ã‚¨ãƒãƒƒã‚¯æ•°**: 5ã‚¨ãƒãƒƒã‚¯ï¼ˆéå­¦ç¿’ã‚’é˜²ããŸã‚ï¼‰\n",
        "- **å­¦ç¿’ç‡**: 0.001ï¼ˆæ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’å£Šã•ãªã„ãŸã‚ï¼‰\n",
        "- **æ—¢å­˜ãƒ¢ãƒ‡ãƒ«**: 1å›ç›®ã®finetuneã§ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ã‚»ãƒ«0: AWSèªè¨¼è¨­å®š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAxxxxxxxxxxxxxxxx\"  # åŸ‹ã‚ã¦ã­ï¼ï¼ï¼ï¼ï¼ï¼ï¼\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"xxxxxxxxxxxxxxxxxxxx\"  # åŸ‹ã‚ã¦ã­ï¼ï¼ï¼ï¼ï¼ï¼ï¼\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] = \"ap-northeast-1\"\n",
        "\n",
        "print(\"AWS env set\")\n",
        "\n",
        "assert \"AWS_ACCESS_KEY_ID\" in os.environ, \"AWS_ACCESS_KEY_ID not set\"\n",
        "assert \"AWS_SECRET_ACCESS_KEY\" in os.environ, \"AWS_SECRET_ACCESS_KEY not set\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ã‚»ãƒ«1: è¨­å®šï¼ˆã“ã“ã ã‘è§¦ã‚Œã°OKï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# è¨­å®šï¼ˆã“ã“ã ã‘å¤‰æ›´ã™ã‚Œã°OKï¼‰\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# ===== S3è¨­å®š =====\n",
        "S3_BUCKET = \"km62m-ml-storage\"  # S3ãƒã‚±ãƒƒãƒˆå\n",
        "S3_PREFIX_PAREN = \"yolo-dataset/finetune2-paren\"  # æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿\n",
        "S3_PREFIX_AIDA = \"yolo-dataset/v3\"  # Aidaãƒ‡ãƒ¼ã‚¿\n",
        "\n",
        "# ===== ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹è¨­å®š =====\n",
        "LOCAL_DATA_DIR = Path(\"/content/tmp/yolo_dataset_finetune2\")  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å±•é–‹å…ˆ\n",
        "LOCAL_OUTPUT_DIR = Path(\"/content/tmp/yolo_training_finetune2\")  # å­¦ç¿’çµæœã®å‡ºåŠ›å…ˆ\n",
        "\n",
        "# ===== å­¦ç¿’è¨­å®š =====\n",
        "EPOCHS = 10  # ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆAidaãƒ‡ãƒ¼ã‚¿çµ±åˆã«ã‚ˆã‚Š10ã«å¢—åŠ ï¼‰\n",
        "BATCH_SIZE = 16  # ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆGPUãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´ï¼‰\n",
        "IMG_SIZE = 640  # ç”»åƒã‚µã‚¤ã‚º\n",
        "DEVICE = 0  # GPUä½¿ç”¨ï¼ˆ0: GPU, 'cpu': CPUï¼‰\n",
        "\n",
        "# ===== ãƒ‡ãƒ¼ã‚¿çµ±åˆè¨­å®š =====\n",
        "# æ–°ãƒ‡ãƒ¼ã‚¿ï¼ˆæ‹¬å¼§ï¼‰: æ—§ãƒ‡ãƒ¼ã‚¿ï¼ˆAidaï¼‰ã®æ¯”ç‡ï¼ˆæ¨å¥¨: 1:3 ã€œ 1:5ï¼‰\n",
        "DATA_RATIO_NEW_TO_OLD = 1 / 4  # 1:4ã®æ¯”ç‡ï¼ˆæ–°:æ—§ï¼‰\n",
        "CLASS_ID_LPAREN = 42  # å·¦æ‹¬å¼§ã®ã‚¯ãƒ©ã‚¹ID\n",
        "CLASS_ID_RPAREN = 43  # å³æ‹¬å¼§ã®ã‚¯ãƒ©ã‚¹ID\n",
        "# Aidaãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„ã‚¯ãƒ©ã‚¹æœ€å¤§ä½•æšã¾ã§ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆNone=å…¨é‡ä½¿ç”¨ï¼‰\n",
        "MAX_SAMPLES_PER_CLASS_AIDA = None  # Noneã¾ãŸã¯æ•°å€¤ï¼ˆä¾‹: 2000ï¼‰\n",
        "\n",
        "# ===== train/valåˆ†å‰²è¨­å®š =====\n",
        "SEED = 42  # å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰\n",
        "VAL_RATIO = 0.2  # 20%ã‚’valã«\n",
        "\n",
        "# ===== ãƒ¢ãƒ‡ãƒ«è¨­å®š =====\n",
        "MODEL_NAME = \"yolov5n\"  # YOLOv5nãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\n",
        "# 1å›ç›®ã®finetuneã§ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«é‡ã¿ï¼ˆS3ã‹ã‚‰å–å¾—ï¼‰\n",
        "PRETRAINED_WEIGHTS_S3 = f\"s3://{S3_BUCKET}/yolo-dataset/v3/weights/yolov5n_train_20251229_161158/best.pt\"  # å®Ÿéš›ã®ãƒ‘ã‚¹ã«ç½®ãæ›ãˆã‚‹\n",
        "PRETRAINED_WEIGHTS_LOCAL = Path(\"/content/tmp/pretrained_weights/best.pt\")  # ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å…ˆ\n",
        "\n",
        "# ===== ãã®ä»– =====\n",
        "CLEAN_LOCAL_DATA = False  # Trueã«ã™ã‚‹ã¨æ—¢å­˜ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤\n",
        "\n",
        "print(\"âœ“ è¨­å®šå®Œäº†\")\n",
        "print(f\"  S3_BUCKET: {S3_BUCKET}\")\n",
        "print(f\"  S3_PREFIX_PAREN: {S3_PREFIX_PAREN}\")\n",
        "print(f\"  S3_PREFIX_AIDA: {S3_PREFIX_AIDA}\")\n",
        "print(f\"  LOCAL_DATA_DIR: {LOCAL_DATA_DIR}\")\n",
        "print(f\"  EPOCHS: {EPOCHS}\")\n",
        "print(f\"  BATCH_SIZE: {BATCH_SIZE}\")\n",
        "print(f\"  IMG_SIZE: {IMG_SIZE}\")\n",
        "print(f\"  VAL_RATIO: {VAL_RATIO}\")\n",
        "print(f\"  DATA_RATIO_NEW_TO_OLD: 1:{int(1/DATA_RATIO_NEW_TO_OLD)} (æ–°:æ—§)\")\n",
        "print(f\"  MAX_SAMPLES_PER_CLASS_AIDA: {MAX_SAMPLES_PER_CLASS_AIDA or 'å…¨é‡'}\")\n",
        "print(f\"  PRETRAINED_WEIGHTS_S3: {PRETRAINED_WEIGHTS_S3}\")\n",
        "print(f\"\\nâ­ æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ã¨Aidaãƒ‡ãƒ¼ã‚¿ã‚’1:{int(1/DATA_RATIO_NEW_TO_OLD)}ã®æ¯”ç‡ã§çµ±åˆã—ã¾ã™\")\n",
        "print(f\"  âš ï¸  Aidaãƒ‡ãƒ¼ã‚¿ã®æ‹¬å¼§ã‚¯ãƒ©ã‚¹ï¼ˆ{CLASS_ID_LPAREN}, {CLASS_ID_RPAREN}ï¼‰ã¯é™¤å¤–ã—ã¾ã™\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ã‚»ãƒ«2: YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ã¨ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "# ============================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‘ã‚¹\n",
        "YOLOV5_DIR = Path(\"/content/yolov5\")\n",
        "\n",
        "# YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ï¼ˆæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
        "if YOLOV5_DIR.exists() and (YOLOV5_DIR / \"train.py\").exists():\n",
        "    print(f\"\\nâœ“ YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã¯æ—¢ã«ã‚¯ãƒ­ãƒ¼ãƒ³æ¸ˆã¿: {YOLOV5_DIR}\")\n",
        "else:\n",
        "    print(f\"\\nğŸ“¦ YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­...\")\n",
        "    print(\"   URL: https://github.com/ultralytics/yolov5\")\n",
        "    \n",
        "    # æ—¢å­˜ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚Œã°å‰Šé™¤\n",
        "    if YOLOV5_DIR.exists():\n",
        "        import shutil\n",
        "        shutil.rmtree(YOLOV5_DIR)\n",
        "    \n",
        "    subprocess.check_call([\n",
        "        \"git\", \"clone\", \n",
        "        \"https://github.com/ultralytics/yolov5.git\",\n",
        "        str(YOLOV5_DIR)\n",
        "    ])\n",
        "    print(f\"âœ“ YOLOv5ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³å®Œäº†\")\n",
        "\n",
        "# YOLOv5ã®ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "print(f\"\\nğŸ“¦ YOLOv5ã®ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
        "requirements_file = YOLOV5_DIR / \"requirements.txt\"\n",
        "if requirements_file.exists():\n",
        "    subprocess.check_call([\n",
        "        sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \n",
        "        \"-r\", str(requirements_file)\n",
        "    ])\n",
        "    print(f\"âœ“ YOLOv5ã®ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
        "else:\n",
        "    print(f\"âš ï¸  è­¦å‘Š: requirements.txtãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "\n",
        "# AWS CLI\n",
        "try:\n",
        "    result = subprocess.run([\"aws\", \"--version\"], capture_output=True, text=True)\n",
        "    print(f\"\\nâœ“ AWS CLI ã¯æ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿: {result.stdout.strip()}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nğŸ“¦ AWS CLI ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
        "    subprocess.check_call([\"pip\", \"install\", \"-q\", \"awscli\"])\n",
        "    print(\"âœ“ AWS CLI ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
        "\n",
        "print(f\"\\nâœ“ YOLOv5ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†\")\n",
        "print(f\"  YOLOv5ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {YOLOV5_DIR}\")\n",
        "print(f\"  train.py: {YOLOV5_DIR / 'train.py'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ã‚»ãƒ«3: S3ã‹ã‚‰æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ã¨Aidaãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»çµ±åˆãƒ»train/valåˆ†å‰²\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# S3ã‹ã‚‰æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ã¨Aidaãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»çµ±åˆãƒ»train/valåˆ†å‰²\n",
        "# ============================================\n",
        "\n",
        "import subprocess\n",
        "import tarfile\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"S3ã‹ã‚‰æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ã¨Aidaãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»çµ±åˆãƒ»train/valåˆ†å‰²\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"â­ ãƒ‡ãƒ¼ã‚¿æ¯”ç‡: æ–°ï¼ˆæ‹¬å¼§ï¼‰: æ—§ï¼ˆAidaï¼‰ = 1:{int(1/DATA_RATIO_NEW_TO_OLD)}\")\n",
        "print(f\"âš ï¸  Aidaãƒ‡ãƒ¼ã‚¿ã®æ‹¬å¼§ã‚¯ãƒ©ã‚¹ï¼ˆ{CLASS_ID_LPAREN}, {CLASS_ID_RPAREN}ï¼‰ã¯é™¤å¤–ã—ã¾ã™\")\n",
        "\n",
        "# æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "if CLEAN_LOCAL_DATA and LOCAL_DATA_DIR.exists():\n",
        "    print(f\"\\nğŸ—‘ï¸  æ—¢å­˜ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤: {LOCAL_DATA_DIR}\")\n",
        "    shutil.rmtree(LOCAL_DATA_DIR)\n",
        "\n",
        "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
        "download_dir = LOCAL_DATA_DIR.parent / \"yolo_tars_download_finetune2\"\n",
        "download_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ\n",
        "LOCAL_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "train_images_dir = LOCAL_DATA_DIR / \"train\" / \"images\"\n",
        "train_labels_dir = LOCAL_DATA_DIR / \"train\" / \"labels\"\n",
        "val_images_dir = LOCAL_DATA_DIR / \"val\" / \"images\"\n",
        "val_labels_dir = LOCAL_DATA_DIR / \"val\" / \"labels\"\n",
        "\n",
        "for d in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ===== æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ =====\n",
        "print(f\"\\nğŸ“¥ æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "paren_tar_files = [\n",
        "    \"paren_train_images.tar\",\n",
        "    \"paren_train_labels.tar\"\n",
        "]\n",
        "\n",
        "for tar_name in paren_tar_files:\n",
        "    s3_path = f\"s3://{S3_BUCKET}/{S3_PREFIX_PAREN}/{tar_name}\"\n",
        "    local_tar_path = download_dir / tar_name\n",
        "    \n",
        "    if local_tar_path.exists():\n",
        "        size_mb = local_tar_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"  âœ“ {tar_name} ã¯æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {size_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"  ğŸ“¥ {tar_name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"aws\", \"s3\", \"cp\", s3_path, str(local_tar_path)],\n",
        "                check=True, capture_output=True\n",
        "            )\n",
        "            size_mb = local_tar_path.stat().st_size / (1024 * 1024)\n",
        "            print(f\"    âœ“ å®Œäº†: {size_mb:.2f} MB\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"    âŒ å¤±æ•—: {e}\")\n",
        "            raise\n",
        "\n",
        "# ===== Aidaãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ =====\n",
        "print(f\"\\nğŸ“¥ Aidaãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "aida_tar_files = [\n",
        "    \"train_images.tar\",\n",
        "    \"train_labels.tar\"\n",
        "]\n",
        "\n",
        "for tar_name in aida_tar_files:\n",
        "    s3_path = f\"s3://{S3_BUCKET}/{S3_PREFIX_AIDA}/{tar_name}\"\n",
        "    local_tar_path = download_dir / tar_name\n",
        "    \n",
        "    if local_tar_path.exists():\n",
        "        size_mb = local_tar_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"  âœ“ {tar_name} ã¯æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {size_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"  ğŸ“¥ {tar_name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"aws\", \"s3\", \"cp\", s3_path, str(local_tar_path)],\n",
        "                check=True, capture_output=True\n",
        "            )\n",
        "            size_mb = local_tar_path.stat().st_size / (1024 * 1024)\n",
        "            print(f\"    âœ“ å®Œäº†: {size_mb:.2f} MB\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"    âŒ å¤±æ•—: {e}\")\n",
        "            raise\n",
        "\n",
        "# ===== æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ã‚’å±•é–‹ =====\n",
        "print(f\"\\nğŸ“¦ æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ã‚’å±•é–‹ä¸­...\")\n",
        "temp_paren_dir = LOCAL_DATA_DIR.parent / \"temp_paren\"\n",
        "temp_paren_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for tar_name in paren_tar_files:\n",
        "    tar_path = download_dir / tar_name\n",
        "    if tar_path.exists():\n",
        "        target_dir = temp_paren_dir / tar_name.replace(\".tar\", \"\").replace(\"paren_\", \"\")\n",
        "        target_dir.mkdir(parents=True, exist_ok=True)\n",
        "        with tarfile.open(tar_path, 'r') as tar:\n",
        "            tar.extractall(target_dir)\n",
        "        print(f\"  âœ“ {tar_name} å±•é–‹å®Œäº†\")\n",
        "\n",
        "# ===== Aidaãƒ‡ãƒ¼ã‚¿ã‚’å±•é–‹ =====\n",
        "print(f\"\\nğŸ“¦ Aidaãƒ‡ãƒ¼ã‚¿ã‚’å±•é–‹ä¸­...\")\n",
        "temp_aida_dir = LOCAL_DATA_DIR.parent / \"temp_aida\"\n",
        "temp_aida_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for tar_name in aida_tar_files:\n",
        "    tar_path = download_dir / tar_name\n",
        "    if tar_path.exists():\n",
        "        target_dir = temp_aida_dir / tar_name.replace(\".tar\", \"\")\n",
        "        target_dir.mkdir(parents=True, exist_ok=True)\n",
        "        with tarfile.open(tar_path, 'r') as tar:\n",
        "            tar.extractall(target_dir)\n",
        "        print(f\"  âœ“ {tar_name} å±•é–‹å®Œäº†\")\n",
        "\n",
        "# ===== æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ =====\n",
        "print(f\"\\nğŸ“Š æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "paren_images = list((temp_paren_dir / \"train_images\").glob(\"*\"))\n",
        "paren_labels = list((temp_paren_dir / \"train_labels\").glob(\"*.txt\"))\n",
        "\n",
        "paren_image_uuids = {f.stem for f in paren_images}\n",
        "paren_label_uuids = {f.stem for f in paren_labels}\n",
        "paren_matched_uuids = sorted(list(paren_image_uuids & paren_label_uuids))\n",
        "\n",
        "print(f\"  æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ç·æ•°: {len(paren_matched_uuids)} ä»¶\")\n",
        "\n",
        "# ===== Aidaãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆæ‹¬å¼§ã‚¯ãƒ©ã‚¹ã‚’é™¤å¤–ï¼‰ =====\n",
        "print(f\"\\nğŸ“Š Aidaãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­ï¼ˆæ‹¬å¼§ã‚¯ãƒ©ã‚¹é™¤å¤–ï¼‰...\")\n",
        "aida_images = list((temp_aida_dir / \"train_images\").glob(\"*\"))\n",
        "aida_labels = list((temp_aida_dir / \"train_labels\").glob(\"*.txt\"))\n",
        "\n",
        "aida_image_uuids = {f.stem for f in aida_images}\n",
        "aida_label_uuids = {f.stem for f in aida_labels}\n",
        "aida_matched_uuids = sorted(list(aida_image_uuids & aida_label_uuids))\n",
        "\n",
        "print(f\"  Aidaãƒ‡ãƒ¼ã‚¿ç·æ•°ï¼ˆãƒãƒƒãƒãƒ³ã‚°å‰ï¼‰: {len(aida_matched_uuids)} ä»¶\")\n",
        "\n",
        "# æ‹¬å¼§ã‚¯ãƒ©ã‚¹ã‚’å«ã‚€ãƒ©ãƒ™ãƒ«ã‚’é™¤å¤–\n",
        "aida_filtered_uuids = []\n",
        "aida_class_counts = defaultdict(int)\n",
        "excluded_count = 0\n",
        "\n",
        "for uuid in tqdm(aida_matched_uuids, desc=\"Aidaãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\"):\n",
        "    label_file = temp_aida_dir / \"train_labels\" / f\"{uuid}.txt\"\n",
        "    if not label_file.exists():\n",
        "        continue\n",
        "    \n",
        "    # ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§æ‹¬å¼§ã‚¯ãƒ©ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
        "    has_paren = False\n",
        "    with open(label_file, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 5:\n",
        "                class_id = int(parts[0])\n",
        "                aida_class_counts[class_id] += 1\n",
        "                if class_id == CLASS_ID_LPAREN or class_id == CLASS_ID_RPAREN:\n",
        "                    has_paren = True\n",
        "                    break\n",
        "    \n",
        "    if not has_paren:\n",
        "        aida_filtered_uuids.append(uuid)\n",
        "    else:\n",
        "        excluded_count += 1\n",
        "\n",
        "print(f\"  æ‹¬å¼§ã‚¯ãƒ©ã‚¹ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–: {excluded_count} ä»¶\")\n",
        "print(f\"  Aidaãƒ‡ãƒ¼ã‚¿ï¼ˆæ‹¬å¼§é™¤å¤–å¾Œï¼‰: {len(aida_filtered_uuids)} ä»¶\")\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚’è¡¨ç¤º\n",
        "print(f\"\\n  Aidaãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒï¼ˆä¸Šä½10ã‚¯ãƒ©ã‚¹ï¼‰:\")\n",
        "sorted_classes = sorted(aida_class_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "for class_id, count in sorted_classes[:10]:\n",
        "    class_name = f\"class_{class_id}\"\n",
        "    if class_id == CLASS_ID_LPAREN:\n",
        "        class_name = \"(\"\n",
        "    elif class_id == CLASS_ID_RPAREN:\n",
        "        class_name = \")\"\n",
        "    print(f\"    ã‚¯ãƒ©ã‚¹{class_id} ({class_name}): {count} ä»¶\")\n",
        "\n",
        "# ===== å„ã‚¯ãƒ©ã‚¹ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆAidaãƒ‡ãƒ¼ã‚¿ï¼‰ =====\n",
        "print(f\"\\nğŸ“Š Aidaãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ©ã‚¹ã”ã¨ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¸­...\")\n",
        "aida_by_class = defaultdict(list)\n",
        "\n",
        "for uuid in aida_filtered_uuids:\n",
        "    label_file = temp_aida_dir / \"train_labels\" / f\"{uuid}.txt\"\n",
        "    if not label_file.exists():\n",
        "        continue\n",
        "    \n",
        "    with open(label_file, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 5:\n",
        "                class_id = int(parts[0])\n",
        "                if class_id != CLASS_ID_LPAREN and class_id != CLASS_ID_RPAREN:\n",
        "                    aida_by_class[class_id].append(uuid)\n",
        "                    break  # 1ã¤ã®ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«1ã¤ã®ã‚¯ãƒ©ã‚¹IDã®ã¿\n",
        "\n",
        "# å„ã‚¯ãƒ©ã‚¹ã‹ã‚‰æœ€å¤§MAX_SAMPLES_PER_CLASS_AIDAæšã¾ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "random.seed(SEED)\n",
        "aida_sampled_uuids = []\n",
        "for class_id, uuids in aida_by_class.items():\n",
        "    if MAX_SAMPLES_PER_CLASS_AIDA is not None:\n",
        "        sampled = random.sample(uuids, min(len(uuids), MAX_SAMPLES_PER_CLASS_AIDA))\n",
        "    else:\n",
        "        sampled = uuids\n",
        "    aida_sampled_uuids.extend(sampled)\n",
        "\n",
        "# é‡è¤‡ã‚’é™¤å»ï¼ˆ1ã¤ã®UUIDãŒè¤‡æ•°ã®ã‚¯ãƒ©ã‚¹ã«å«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰\n",
        "aida_sampled_uuids = list(set(aida_sampled_uuids))\n",
        "random.shuffle(aida_sampled_uuids)\n",
        "\n",
        "print(f\"  ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: {len(aida_sampled_uuids)} ä»¶\")\n",
        "\n",
        "# ===== æ¯”ç‡ã«åˆã‚ã›ã¦Aidaãƒ‡ãƒ¼ã‚¿ã‚’èª¿æ•´ =====\n",
        "num_paren = len(paren_matched_uuids)\n",
        "target_aida = int(num_paren / DATA_RATIO_NEW_TO_OLD)\n",
        "\n",
        "if len(aida_sampled_uuids) > target_aida:\n",
        "    aida_sampled_uuids = aida_sampled_uuids[:target_aida]\n",
        "    print(f\"  æ¯”ç‡èª¿æ•´å¾Œ: {len(aida_sampled_uuids)} ä»¶ï¼ˆç›®æ¨™: {target_aida} ä»¶ï¼‰\")\n",
        "else:\n",
        "    print(f\"  å…¨é‡ä½¿ç”¨: {len(aida_sampled_uuids)} ä»¶ï¼ˆç›®æ¨™: {target_aida} ä»¶ï¼‰\")\n",
        "\n",
        "print(f\"\\n  æœ€çµ‚ãƒ‡ãƒ¼ã‚¿æ•°:\")\n",
        "print(f\"    æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿: {num_paren} ä»¶\")\n",
        "print(f\"    Aidaãƒ‡ãƒ¼ã‚¿: {len(aida_sampled_uuids)} ä»¶\")\n",
        "print(f\"    åˆè¨ˆ: {num_paren + len(aida_sampled_uuids)} ä»¶\")\n",
        "print(f\"    æ¯”ç‡: 1:{len(aida_sampled_uuids)/num_paren:.2f} (æ–°:æ—§)\")\n",
        "\n",
        "# ===== train/valåˆ†å‰² =====\n",
        "print(f\"\\nğŸ“Š train/valåˆ†å‰²ä¸­...\")\n",
        "\n",
        "# æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²\n",
        "random.seed(SEED)\n",
        "random.shuffle(paren_matched_uuids)\n",
        "paren_split_idx = int(len(paren_matched_uuids) * (1 - VAL_RATIO))\n",
        "paren_train_uuids = paren_matched_uuids[:paren_split_idx]\n",
        "paren_val_uuids = paren_matched_uuids[paren_split_idx:]\n",
        "\n",
        "# Aidaãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²\n",
        "random.seed(SEED)\n",
        "random.shuffle(aida_sampled_uuids)\n",
        "aida_split_idx = int(len(aida_sampled_uuids) * (1 - VAL_RATIO))\n",
        "aida_train_uuids = aida_sampled_uuids[:aida_split_idx]\n",
        "aida_val_uuids = aida_sampled_uuids[aida_split_idx:]\n",
        "\n",
        "print(f\"  æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ - Train: {len(paren_train_uuids)} ä»¶, Val: {len(paren_val_uuids)} ä»¶\")\n",
        "print(f\"  Aidaãƒ‡ãƒ¼ã‚¿ - Train: {len(aida_train_uuids)} ä»¶, Val: {len(aida_val_uuids)} ä»¶\")\n",
        "\n",
        "# ===== ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ =====\n",
        "print(f\"\\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ä¸­...\")\n",
        "\n",
        "# æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ï¼ˆtrainï¼‰\n",
        "for uuid in tqdm(paren_train_uuids, desc=\"æ‹¬å¼§train\"):\n",
        "    img_files = list((temp_paren_dir / \"train_images\").glob(f\"{uuid}.*\"))\n",
        "    if img_files:\n",
        "        shutil.copy2(img_files[0], train_images_dir / img_files[0].name)\n",
        "    label_file = temp_paren_dir / \"train_labels\" / f\"{uuid}.txt\"\n",
        "    if label_file.exists():\n",
        "        shutil.copy2(label_file, train_labels_dir / f\"{uuid}.txt\")\n",
        "\n",
        "# æ‹¬å¼§ãƒ‡ãƒ¼ã‚¿ï¼ˆvalï¼‰\n",
        "for uuid in tqdm(paren_val_uuids, desc=\"æ‹¬å¼§val\"):\n",
        "    img_files = list((temp_paren_dir / \"train_images\").glob(f\"{uuid}.*\"))\n",
        "    if img_files:\n",
        "        shutil.copy2(img_files[0], val_images_dir / img_files[0].name)\n",
        "    label_file = temp_paren_dir / \"train_labels\" / f\"{uuid}.txt\"\n",
        "    if label_file.exists():\n",
        "        shutil.copy2(label_file, val_labels_dir / f\"{uuid}.txt\")\n",
        "\n",
        "# Aidaãƒ‡ãƒ¼ã‚¿ï¼ˆtrainï¼‰\n",
        "for uuid in tqdm(aida_train_uuids, desc=\"Aida train\"):\n",
        "    img_files = list((temp_aida_dir / \"train_images\").glob(f\"{uuid}.*\"))\n",
        "    if img_files:\n",
        "        shutil.copy2(img_files[0], train_images_dir / img_files[0].name)\n",
        "    label_file = temp_aida_dir / \"train_labels\" / f\"{uuid}.txt\"\n",
        "    if label_file.exists():\n",
        "        shutil.copy2(label_file, train_labels_dir / f\"{uuid}.txt\")\n",
        "\n",
        "# Aidaãƒ‡ãƒ¼ã‚¿ï¼ˆvalï¼‰\n",
        "for uuid in tqdm(aida_val_uuids, desc=\"Aida val\"):\n",
        "    img_files = list((temp_aida_dir / \"train_images\").glob(f\"{uuid}.*\"))\n",
        "    if img_files:\n",
        "        shutil.copy2(img_files[0], val_images_dir / img_files[0].name)\n",
        "    label_file = temp_aida_dir / \"train_labels\" / f\"{uuid}.txt\"\n",
        "    if label_file.exists():\n",
        "        shutil.copy2(label_file, val_labels_dir / f\"{uuid}.txt\")\n",
        "\n",
        "# ===== çµæœã®ç¢ºèª =====\n",
        "train_images = list(train_images_dir.glob(\"*\"))\n",
        "train_labels = list(train_labels_dir.glob(\"*.txt\"))\n",
        "val_images = list(val_images_dir.glob(\"*\"))\n",
        "val_labels = list(val_labels_dir.glob(\"*.txt\"))\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"  Trainç”»åƒ: {len(train_images)} ä»¶\")\n",
        "print(f\"  Trainãƒ©ãƒ™ãƒ«: {len(train_labels)} ä»¶\")\n",
        "print(f\"  Valç”»åƒ: {len(val_images)} ä»¶\")\n",
        "print(f\"  Valãƒ©ãƒ™ãƒ«: {len(val_labels)} ä»¶\")\n",
        "print(f\"\\nâœ“ ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†ï¼ˆæ‹¬å¼§ + Aidaã€æ¯”ç‡1:{int(1/DATA_RATIO_NEW_TO_OLD)}ï¼‰\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ã‚»ãƒ«4: data.yamlã¨hyp.yamlã®ä½œæˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# data.yamlã¨hyp.yamlã®ä½œæˆ\n",
        "# ============================================\n",
        "\n",
        "import yaml\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"data.yamlã¨hyp.yamlä½œæˆ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹å®šç¾©ï¼ˆæ—¢å­˜ã®YOLOãƒ¢ãƒ‡ãƒ«ã¨ä¸€è‡´ï¼‰\n",
        "# python/data/config/classes.pyã‹ã‚‰å–å¾—\n",
        "target_classes = [\n",
        "    # æ•°å­—ï¼ˆ10ã‚¯ãƒ©ã‚¹ï¼‰\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "    # å°æ•°ç‚¹ï¼ˆ1ã‚¯ãƒ©ã‚¹ï¼‰\n",
        "    '.',\n",
        "    # åŸºæœ¬æ¼”ç®—å­ï¼ˆ5ã‚¯ãƒ©ã‚¹ï¼‰\n",
        "    '+', '-', '*', '/', '=',\n",
        "    # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆå¤‰æ•°ï¼ˆå°æ–‡å­—26ã‚¯ãƒ©ã‚¹ï¼‰\n",
        "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
        "    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
        "    # æ‹¬å¼§é¡ï¼ˆ3ã‚¯ãƒ©ã‚¹ï¼‰\n",
        "    '(', ')', '|',\n",
        "    # ã‚®ãƒªã‚·ãƒ£æ–‡å­—ãƒ»å®šæ•°ï¼ˆ3ã‚¯ãƒ©ã‚¹ï¼‰\n",
        "    'Ï€', 'Î¸', 'âˆ',\n",
        "    # ç‰¹æ®Šè¨˜å·ï¼ˆ2ã‚¯ãƒ©ã‚¹ï¼‰\n",
        "    'â†’', 'âˆš',\n",
        "    # é–¢æ•°åï¼ˆ12ã‚¯ãƒ©ã‚¹ï¼‰\n",
        "    'sin', 'cos', 'tan', 'sec', 'csc', 'cot', 'ln', 'log', 'exp', 'sqrt', 'lim', 'abs',\n",
        "]\n",
        "\n",
        "print(f\"  ã‚¯ãƒ©ã‚¹æ•°: {len(target_classes)} ã‚¯ãƒ©ã‚¹\")\n",
        "print(f\"  æ‹¬å¼§ã‚¯ãƒ©ã‚¹ID: '('={target_classes.index('(')}, ')'={target_classes.index(')')}\")\n",
        "\n",
        "# data.yamlã‚’ä½œæˆ\n",
        "data_config = {\n",
        "    'path': str(LOCAL_DATA_DIR.absolute()),\n",
        "    'train': 'train/images',\n",
        "    'val': 'val/images',\n",
        "    'nc': len(target_classes),\n",
        "    'names': {idx: cls for idx, cls in enumerate(target_classes)}\n",
        "}\n",
        "\n",
        "data_yaml_file = LOCAL_DATA_DIR / \"data.yaml\"\n",
        "with open(data_yaml_file, 'w', encoding='utf-8') as f:\n",
        "    yaml.dump(data_config, f, default_flow_style=False, allow_unicode=True)\n",
        "\n",
        "print(f\"\\nâœ“ data.yamlã‚’ä½œæˆ: {data_yaml_file}\")\n",
        "\n",
        "# hyp.yamlã‚’ä½œæˆï¼ˆfliplr=0.0è¨­å®šï¼‰\n",
        "# YOLOv5ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®hyp.yamlã‚’ãƒ™ãƒ¼ã‚¹ã«ã€fliplr=0.0ã‚’è¨­å®š\n",
        "hyp_config = {\n",
        "    'lr0': 0.001,  # åˆæœŸå­¦ç¿’ç‡ï¼ˆæ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’å£Šã•ãªã„ãŸã‚0.001ã«å¤‰æ›´ï¼‰\n",
        "    'lrf': 0.1,   # æœ€çµ‚å­¦ç¿’ç‡ï¼ˆlr0 * lrfï¼‰\n",
        "    'momentum': 0.937,\n",
        "    'weight_decay': 0.0005,\n",
        "    'warmup_epochs': 3.0,\n",
        "    'warmup_momentum': 0.8,\n",
        "    'warmup_bias_lr': 0.1,\n",
        "    'box': 0.05,\n",
        "    'cls': 0.5,\n",
        "    'cls_pw': 1.0,\n",
        "    'obj': 1.0,\n",
        "    'obj_pw': 1.0,\n",
        "    'iou_t': 0.20,\n",
        "    'anchor_t': 4.0,\n",
        "    'fl_gamma': 0.0,\n",
        "    'hsv_h': 0.015,  # ç”»åƒHSV-Hue augmentation\n",
        "    'hsv_s': 0.7,    # ç”»åƒHSV-Saturation augmentation\n",
        "    'hsv_v': 0.4,    # ç”»åƒHSV-Value augmentation\n",
        "    'degrees': 0.0,  # ç”»åƒå›è»¢ï¼ˆ+/-åº¦ï¼‰\n",
        "    'translate': 0.1,  # ç”»åƒå¹³è¡Œç§»å‹•ï¼ˆ+/- fractionï¼‰\n",
        "    'scale': 0.5,     # ç”»åƒã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆ+/- gainï¼‰\n",
        "    'shear': 0.0,     # ç”»åƒã›ã‚“æ–­ï¼ˆ+/-åº¦ï¼‰\n",
        "    'perspective': 0.0,  # ç”»åƒé è¿‘æ³•ï¼ˆ+/- fractionï¼‰\n",
        "    'flipud': 0.0,    # ç”»åƒä¸Šä¸‹åè»¢ï¼ˆç¢ºç‡ï¼‰\n",
        "    'fliplr': 0.0,    # â­ ç”»åƒå·¦å³åè»¢ï¼ˆç¢ºç‡ï¼‰â† 0.0ã«è¨­å®šï¼\n",
        "    'mosaic': 1.0,    # ç”»åƒãƒ¢ã‚¶ã‚¤ã‚¯ï¼ˆç¢ºç‡ï¼‰\n",
        "    'mixup': 0.0,     # ç”»åƒãƒŸãƒƒã‚¯ã‚¹ã‚¢ãƒƒãƒ—ï¼ˆç¢ºç‡ï¼‰\n",
        "    'copy_paste': 0.0  # ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ”ãƒ¼ï¼†ãƒšãƒ¼ã‚¹ãƒˆï¼ˆç¢ºç‡ï¼‰\n",
        "}\n",
        "\n",
        "hyp_yaml_file = LOCAL_DATA_DIR / \"hyp.yaml\"\n",
        "with open(hyp_yaml_file, 'w', encoding='utf-8') as f:\n",
        "    yaml.dump(hyp_config, f, default_flow_style=False, allow_unicode=True)\n",
        "\n",
        "print(f\"\\nâœ“ hyp.yamlã‚’ä½œæˆ: {hyp_yaml_file}\")\n",
        "print(f\"  â­ fliplr=0.0 ã‚’è¨­å®šã—ã¾ã—ãŸï¼ˆå·¦å³åè»¢ãªã—ï¼‰\")\n",
        "\n",
        "print(f\"\\nâœ“ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†\")\n",
        "data_path = str(data_yaml_file)\n",
        "hyp_path = str(hyp_yaml_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ã‚»ãƒ«5: äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "# ============================================\n",
        "\n",
        "import subprocess\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å…ˆã®æº–å‚™\n",
        "PRETRAINED_WEIGHTS_LOCAL.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if PRETRAINED_WEIGHTS_LOCAL.exists():\n",
        "    size_mb = PRETRAINED_WEIGHTS_LOCAL.stat().st_size / (1024 * 1024)\n",
        "    print(f\"\\nâœ“ äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {size_mb:.2f} MB\")\n",
        "else:\n",
        "    print(f\"\\nğŸ“¥ äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "    print(f\"  S3ãƒ‘ã‚¹: {PRETRAINED_WEIGHTS_S3}\")\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [\"aws\", \"s3\", \"cp\", PRETRAINED_WEIGHTS_S3, str(PRETRAINED_WEIGHTS_LOCAL)],\n",
        "            check=True\n",
        "        )\n",
        "        size_mb = PRETRAINED_WEIGHTS_LOCAL.stat().st_size / (1024 * 1024)\n",
        "        print(f\"  âœ“ å®Œäº†: {size_mb:.2f} MB\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"  âŒ å¤±æ•—: {e}\")\n",
        "        print(f\"  âš ï¸  æ³¨æ„: PRETRAINED_WEIGHTS_S3ã®ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
        "        raise\n",
        "\n",
        "pretrained_weights_path = str(PRETRAINED_WEIGHTS_LOCAL)\n",
        "print(f\"\\nâœ“ äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™å®Œäº†\")\n",
        "print(f\"  ãƒ‘ã‚¹: {pretrained_weights_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ã‚»ãƒ«6: Finetuneã®å®Ÿè¡Œ\n",
        "\n",
        "**æ³¨æ„**: ã“ã®ã‚»ãƒ«ã¯é•·æ™‚é–“ã‹ã‹ã‚Šã¾ã™ã€‚GPUãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Finetuneã®å®Ÿè¡Œï¼ˆYOLOv5 train.pyï¼‰â€»é€²æ—ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# âœ… wandbã®å¯¾è©±å¾…ã¡ã‚’ç„¡åŠ¹åŒ–ï¼ˆ30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ï¼‰\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Finetuneé–‹å§‹ï¼ˆYOLOv5 train.pyï¼‰\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print()\n",
        "\n",
        "# YOLOv5ã®train.pyã®ãƒ‘ã‚¹\n",
        "train_script = YOLOV5_DIR / \"train.py\"\n",
        "if not train_script.exists():\n",
        "    raise FileNotFoundError(f\"train.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {train_script}\")\n",
        "\n",
        "# å‡ºåŠ›å…ˆã®è¨­å®š\n",
        "LOCAL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "run_name = f\"{MODEL_NAME}_finetune2_{timestamp}\"\n",
        "\n",
        "# train.pyã®å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰\n",
        "train_cmd = [\n",
        "    sys.executable,\n",
        "    str(train_script),\n",
        "    \"--data\", data_path,\n",
        "    \"--hyp\", hyp_path,  # â­ hyp.yamlã‚’æŒ‡å®šï¼ˆfliplr=0.0ï¼‰\n",
        "    \"--weights\", pretrained_weights_path,  # äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿\n",
        "    \"--epochs\", str(EPOCHS),\n",
        "    \"--batch-size\", str(BATCH_SIZE),\n",
        "    \"--img\", str(IMG_SIZE),\n",
        "    \"--device\", str(DEVICE),\n",
        "    \"--project\", str(LOCAL_OUTPUT_DIR),\n",
        "    \"--name\", run_name,\n",
        "    \"--save-period\", \"5\",\n",
        "]\n",
        "\n",
        "print(\"å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰:\")\n",
        "print(f\"  {' '.join(train_cmd)}\")\n",
        "print()\n",
        "\n",
        "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’YOLOv5ã«å¤‰æ›´ã—ã¦å®Ÿè¡Œ\n",
        "original_cwd = Path.cwd()\n",
        "try:\n",
        "    os.chdir(YOLOV5_DIR)\n",
        "    print(f\"ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {YOLOV5_DIR}\")\n",
        "    print()\n",
        "\n",
        "    # âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºã§èµ·å‹•\n",
        "    process = subprocess.Popen(\n",
        "        train_cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1,\n",
        "        universal_newlines=True,\n",
        "    )\n",
        "\n",
        "    # é€²æ—ãƒ­ã‚°ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º\n",
        "    assert process.stdout is not None\n",
        "    for line in process.stdout:\n",
        "        print(line, end=\"\")\n",
        "\n",
        "    returncode = process.wait()\n",
        "    if returncode != 0:\n",
        "        raise subprocess.CalledProcessError(returncode, train_cmd)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Finetuneå®Œäº†\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"çµ‚äº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    \n",
        "    # å®Ÿéš›ã«ä½œæˆã•ã‚ŒãŸrunãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç‰¹å®š\n",
        "    def resolve_actual_run_dir(project_dir: Path, name_prefix: str) -> Path:\n",
        "        \"\"\"name_prefixã§å§‹ã¾ã‚‹runãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¸­ã§æœ€æ–°ã®ã‚‚ã®ã‚’è¿”ã™\"\"\"\n",
        "        candidates = [p for p in project_dir.glob(f\"{name_prefix}*\") if p.is_dir()]\n",
        "        if not candidates:\n",
        "            raise FileNotFoundError(f\"run dir not found under {project_dir} (prefix={name_prefix})\")\n",
        "        return max(candidates, key=lambda p: p.stat().st_mtime)\n",
        "    \n",
        "    actual_results_dir = resolve_actual_run_dir(LOCAL_OUTPUT_DIR, run_name)\n",
        "    results_dir = actual_results_dir\n",
        "    weights_dir = results_dir / \"weights\"\n",
        "    \n",
        "    print(\"\\n[run dir resolved]\")\n",
        "    print(f\"  å®Ÿéš›ã®ä¿å­˜å…ˆ: {results_dir}\")\n",
        "    print(f\"  é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«: {weights_dir}\")\n",
        "    \n",
        "    # å¾Œç¶šã‚»ãƒ«ã§ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ç’°å¢ƒå¤‰æ•°ã«ã‚‚ä¿å­˜\n",
        "    os.environ[\"YOLO_RESULTS_DIR\"] = str(results_dir)\n",
        "    os.environ[\"YOLO_WEIGHTS_DIR\"] = str(weights_dir)\n",
        "\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"\\nâŒ ã‚¨ãƒ©ãƒ¼: Finetuneä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ\")\n",
        "    print(f\"   çµ‚äº†ã‚³ãƒ¼ãƒ‰: {e.returncode}\")\n",
        "    raise\n",
        "\n",
        "finally:\n",
        "    os.chdir(original_cwd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ã‚»ãƒ«7: å­¦ç¿’çµæœã®ç¢ºèª\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# å­¦ç¿’çµæœã®ç¢ºèª\n",
        "# ============================================\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# å­¦ç¿’ã‚»ãƒ«ã§è§£æ±ºæ¸ˆã¿ã®ãƒ‘ã‚¹ã‚’å„ªå…ˆ\n",
        "if \"YOLO_WEIGHTS_DIR\" in os.environ:\n",
        "    weights_dir = Path(os.environ[\"YOLO_WEIGHTS_DIR\"])\n",
        "    results_dir = weights_dir.parent\n",
        "else:\n",
        "    results_dir = LOCAL_OUTPUT_DIR / run_name\n",
        "    weights_dir = results_dir / \"weights\"\n",
        "\n",
        "print(f\"results_dir = {results_dir}\")\n",
        "print(f\"weights_dir = {weights_dir}\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"å­¦ç¿’çµæœ\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {results_dir}\")\n",
        "\n",
        "# é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
        "if weights_dir.exists():\n",
        "    weight_files = list(weights_dir.glob('*.pt'))\n",
        "    print(f\"\\né‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(weight_files)} ä»¶\")\n",
        "    \n",
        "    for wf in sorted(weight_files):\n",
        "        size_mb = wf.stat().st_size / (1024 * 1024)\n",
        "        print(f\"  {wf.name}: {size_mb:.2f} MB\")\n",
        "    \n",
        "    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«\n",
        "    best_model = weights_dir / \"best.pt\"\n",
        "    if best_model.exists():\n",
        "        print(f\"\\nâœ“ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model.name}\")\n",
        "        print(f\"  ã‚µã‚¤ã‚º: {best_model.stat().st_size / (1024 * 1024):.2f} MB\")\n",
        "    \n",
        "    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«\n",
        "    last_model = weights_dir / \"last.pt\"\n",
        "    if last_model.exists():\n",
        "        print(f\"âœ“ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«: {last_model.name}\")\n",
        "        print(f\"  ã‚µã‚¤ã‚º: {last_model.stat().st_size / (1024 * 1024):.2f} MB\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "\n",
        "# å­¦ç¿’æ›²ç·šã®ç¢ºèª\n",
        "results_csv = results_dir / \"results.csv\"\n",
        "if results_csv.exists():\n",
        "    print(f\"\\nå­¦ç¿’æ›²ç·šãƒ‡ãƒ¼ã‚¿: {results_csv}\")\n",
        "\n",
        "# ãƒ—ãƒ­ãƒƒãƒˆç”»åƒã®ç¢ºèª\n",
        "plot_files = list(results_dir.glob(\"*.png\"))\n",
        "if len(plot_files) > 0:\n",
        "    print(f\"\\nãƒ—ãƒ­ãƒƒãƒˆç”»åƒæ•°: {len(plot_files)} ä»¶\")\n",
        "    for pf in plot_files:\n",
        "        print(f\"  {pf.name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ã‚»ãƒ«8: ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "# ============================================\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# å­¦ç¿’ã‚»ãƒ«ã§è§£æ±ºæ¸ˆã¿ã®ãƒ‘ã‚¹ã‚’å„ªå…ˆ\n",
        "if \"YOLO_WEIGHTS_DIR\" in os.environ:\n",
        "    weights_dir = Path(os.environ[\"YOLO_WEIGHTS_DIR\"])\n",
        "    results_dir = weights_dir.parent\n",
        "else:\n",
        "    results_dir = LOCAL_OUTPUT_DIR / run_name\n",
        "    weights_dir = results_dir / \"weights\"\n",
        "\n",
        "print(f\"results_dir = {results_dir}\")\n",
        "print(f\"weights_dir = {weights_dir}\")\n",
        "\n",
        "# AWSèªè¨¼æƒ…å ±ã®ç¢ºèª\n",
        "try:\n",
        "    result = subprocess.run([\"aws\", \"sts\", \"get-caller-identity\"], \n",
        "                          capture_output=True, text=True, check=True)\n",
        "    print(f\"\\nâœ“ AWSèªè¨¼æƒ…å ±ã‚’ç¢ºèª\")\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"\\nâŒ ã‚¨ãƒ©ãƒ¼: AWSèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
        "    print(\"   ã‚»ãƒ«0ã§AWSèªè¨¼æƒ…å ±ã‚’è¨­å®šã—ã¦ãã ã•ã„\")\n",
        "    raise\n",
        "\n",
        "# S3ã®ä¿å­˜å…ˆãƒ‘ã‚¹\n",
        "actual_run_name = results_dir.name\n",
        "s3_weights_prefix = f\"yolo-dataset/finetune2-weights/{actual_run_name}\"\n",
        "\n",
        "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ\n",
        "files_to_upload = []\n",
        "\n",
        "# 1. æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ï¼ˆbest.ptï¼‰\n",
        "best_model = weights_dir / \"best.pt\"\n",
        "if best_model.exists():\n",
        "    files_to_upload.append((\"æœ€è‰¯ãƒ¢ãƒ‡ãƒ«\", best_model, f\"{s3_weights_prefix}/best.pt\"))\n",
        "else:\n",
        "    print(\"âš ï¸  æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ï¼ˆbest.ptï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "\n",
        "# 2. æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ï¼ˆlast.ptï¼‰\n",
        "last_model = weights_dir / \"last.pt\"\n",
        "if last_model.exists():\n",
        "    files_to_upload.append((\"æœ€çµ‚ãƒ¢ãƒ‡ãƒ«\", last_model, f\"{s3_weights_prefix}/last.pt\"))\n",
        "\n",
        "# 3. å­¦ç¿’æ›²ç·šãƒ‡ãƒ¼ã‚¿ï¼ˆresults.csvï¼‰\n",
        "results_csv = results_dir / \"results.csv\"\n",
        "if results_csv.exists():\n",
        "    files_to_upload.append((\"å­¦ç¿’æ›²ç·šãƒ‡ãƒ¼ã‚¿\", results_csv, f\"{s3_weights_prefix}/results.csv\"))\n",
        "\n",
        "# 4. data.yamlï¼ˆå­¦ç¿’ã«ä½¿ç”¨ã—ãŸè¨­å®šï¼‰\n",
        "if data_yaml_file.exists():\n",
        "    files_to_upload.append((\"data.yaml\", data_yaml_file, f\"{s3_weights_prefix}/data.yaml\"))\n",
        "\n",
        "# 5. hyp.yamlï¼ˆfliplr=0.0è¨­å®šï¼‰\n",
        "if hyp_yaml_file.exists():\n",
        "    files_to_upload.append((\"hyp.yaml\", hyp_yaml_file, f\"{s3_weights_prefix}/hyp.yaml\"))\n",
        "\n",
        "print(f\"\\nã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files_to_upload)} ä»¶\")\n",
        "print(f\"S3ãƒ‘ã‚¹: s3://{S3_BUCKET}/{s3_weights_prefix}/\")\n",
        "print()\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "uploaded = []\n",
        "failed = []\n",
        "\n",
        "for desc, local_file, s3_key in files_to_upload:\n",
        "    if not local_file.exists():\n",
        "        print(f\"âš ï¸  ã‚¹ã‚­ãƒƒãƒ—: {desc}ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼‰\")\n",
        "        continue\n",
        "    \n",
        "    s3_path = f\"s3://{S3_BUCKET}/{s3_key}\"\n",
        "    size_mb = local_file.stat().st_size / (1024 * 1024)\n",
        "    \n",
        "    print(f\"\\nğŸ“¤ {desc} ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "    print(f\"   ãƒ•ã‚¡ã‚¤ãƒ«: {local_file.name}\")\n",
        "    print(f\"   ã‚µã‚¤ã‚º: {size_mb:.2f} MB\")\n",
        "    print(f\"   S3ãƒ‘ã‚¹: {s3_path}\")\n",
        "    \n",
        "    max_retries = 3\n",
        "    retry_count = 0\n",
        "    success = False\n",
        "    \n",
        "    while retry_count < max_retries and not success:\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            result = subprocess.run(\n",
        "                [\"aws\", \"s3\", \"cp\", str(local_file), s3_path],\n",
        "                capture_output=True, text=True, check=True\n",
        "            )\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"  âœ“ å®Œäº†: {elapsed:.1f}ç§’\")\n",
        "            uploaded.append(desc)\n",
        "            success = True\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            retry_count += 1\n",
        "            if retry_count < max_retries:\n",
        "                print(f\"  âš ï¸  ãƒªãƒˆãƒ©ã‚¤ {retry_count}/{max_retries}...\")\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print(f\"  âŒ å¤±æ•—: {e.stderr}\")\n",
        "                failed.append(desc)\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"  æˆåŠŸ: {len(uploaded)} ä»¶\")\n",
        "if uploaded:\n",
        "    for name in uploaded:\n",
        "        print(f\"    âœ“ {name}\")\n",
        "\n",
        "print(f\"  å¤±æ•—: {len(failed)} ä»¶\")\n",
        "if failed:\n",
        "    for name in failed:\n",
        "        print(f\"    âŒ {name}\")\n",
        "\n",
        "if len(uploaded) == len(files_to_upload):\n",
        "    print(f\"\\nâœ“ å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
        "    print(f\"  S3ãƒ‘ã‚¹: s3://{S3_BUCKET}/{s3_weights_prefix}/\")\n",
        "    print(f\"\\næ¬¡å›ã¯ä»¥ä¸‹ã®ãƒ‘ã‚¹ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚ã¾ã™:\")\n",
        "    print(f\"  æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: s3://{S3_BUCKET}/{s3_weights_prefix}/best.pt\")\n",
        "    print(f\"\\nâ­ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ fliplr=0.0 ã§å­¦ç¿’ã•ã‚Œã¦ã„ã¾ã™ï¼ˆæ‹¬å¼§ã®è­˜åˆ¥å¢ƒç•ŒãŒçŸ¯æ­£æ¸ˆã¿ï¼‰\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸  ä¸€éƒ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
        "    print(f\"  å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯æ‰‹å‹•ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
